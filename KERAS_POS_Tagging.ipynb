{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KERAS POS-Tagging.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miweru/colab-experiments/blob/master/KERAS_POS_Tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ynvR-7D8bwss",
        "colab_type": "code",
        "outputId": "73fc081a-7d7c-4f41-a2cc-bc1363ede4cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "cell_type": "code",
      "source": [
        "#Das hier sind unsere Trainingsdaten.\n",
        "! git clone https://github.com/UniversalDependencies/UD_German-GSD.git\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'UD_German-GSD'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects:  12% (1/8)   \u001b[K\rremote: Counting objects:  25% (2/8)   \u001b[K\rremote: Counting objects:  37% (3/8)   \u001b[K\rremote: Counting objects:  50% (4/8)   \u001b[K\rremote: Counting objects:  62% (5/8)   \u001b[K\rremote: Counting objects:  75% (6/8)   \u001b[K\rremote: Counting objects:  87% (7/8)   \u001b[K\rremote: Counting objects: 100% (8/8)   \u001b[K\rremote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 437 (delta 2), reused 5 (delta 1), pack-reused 429\u001b[K\n",
            "Receiving objects: 100% (437/437), 18.45 MiB | 15.42 MiB/s, done.\n",
            "Resolving deltas: 100% (280/280), done.\n",
            "sample_data  UD_German-GSD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4AKKLaaqlOa6",
        "colab_type": "code",
        "outputId": "3381abbf-f1f6-4e79-eb88-1df89e147567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 23256\n",
            "drwxr-xr-x 2 root root     4096 Dec  4 21:01 sample_data\n",
            "-rw-r--r-- 1 root root 23805304 Dec  6 08:22 Savedvecs.gz\n",
            "drwxr-xr-x 4 root root     4096 Dec  6 08:05 UD_German-GSD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9avVBCg1cQee",
        "colab_type": "code",
        "outputId": "1e545328-bd08-4b7d-b484-b542c4ee0276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3673
        }
      },
      "cell_type": "code",
      "source": [
        "for ln, line in enumerate(open(\"UD_German-GSD/de_gsd-ud-train.conllu\")):\n",
        "  if ln>100:\n",
        "    break\n",
        "  print(line)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# sent_id = train-s1\n",
            "\n",
            "# text = Sehr gute Beratung, schnelle Behebung der Probleme, so stelle ich mir Kundenservice vor.\n",
            "\n",
            "1\tSehr\tsehr\tADV\tADV\t_\t2\tadvmod\t_\t_\n",
            "\n",
            "2\tgute\tgut\tADJ\tADJA\tCase=Nom|Gender=Fem|Number=Sing\t3\tamod\t_\t_\n",
            "\n",
            "3\tBeratung\tBeratung\tNOUN\tNN\tCase=Nom|Gender=Fem|Number=Sing\t0\troot\t_\tSpaceAfter=No\n",
            "\n",
            "4\t,\t,\tPUNCT\t$,\t_\t6\tpunct\t_\t_\n",
            "\n",
            "5\tschnelle\tschnell\tADJ\tADJA\tCase=Nom|Gender=Fem|Number=Sing\t6\tamod\t_\t_\n",
            "\n",
            "6\tBehebung\tBehebung\tNOUN\tNN\tCase=Nom|Gender=Fem|Number=Sing\t3\tconj\t_\t_\n",
            "\n",
            "7\tder\tder\tDET\tART\tCase=Gen|Definite=Def|Gender=Neut|Number=Plur|PronType=Art\t8\tdet\t_\t_\n",
            "\n",
            "8\tProbleme\tProblem\tNOUN\tNN\tCase=Gen|Gender=Neut|Number=Plur\t6\tnmod\t_\tSpaceAfter=No\n",
            "\n",
            "9\t,\t,\tPUNCT\t$,\t_\t11\tpunct\t_\t_\n",
            "\n",
            "10\tso\tso\tADV\tADV\t_\t11\tadvmod\t_\t_\n",
            "\n",
            "11\tstelle\tstellen\tVERB\tVVFIN\tMood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin\t3\tparataxis\t_\t_\n",
            "\n",
            "12\tich\tich\tPRON\tPPER\tCase=Nom|Number=Sing|Person=1|PronType=Prs\t11\tnsubj\t_\t_\n",
            "\n",
            "13\tmir\tich\tPRON\tPRF\tCase=Dat|Number=Sing|Person=1|PronType=Prs|Reflex=Yes\t11\tiobj\t_\t_\n",
            "\n",
            "14\tKundenservice\tKundenservice\tNOUN\tNN\tCase=Acc|Gender=Neut|Number=Sing\t11\tobj\t_\t_\n",
            "\n",
            "15\tvor\tvor\tADP\tPTKVZ\t_\t11\tcompound:prt\t_\tSpaceAfter=No\n",
            "\n",
            "16\t.\t.\tPUNCT\t$.\t_\t3\tpunct\t_\t_\n",
            "\n",
            "\n",
            "\n",
            "# sent_id = train-s2\n",
            "\n",
            "# text = Die Kosten sind definitiv auch im Rahmen.\n",
            "\n",
            "1\tDie\tder\tDET\tART\tCase=Nom|Number=Plur|PronType=Art\t2\tdet\t_\t_\n",
            "\n",
            "2\tKosten\tKosten\tNOUN\tNN\tCase=Nom|Number=Plur\t3\tnsubj:pass\t_\t_\n",
            "\n",
            "3\tsind\tsein\tVERB\tVAFIN\tMood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin\t0\troot\t_\t_\n",
            "\n",
            "4\tdefinitiv\tdefinitiv\tADV\tADJD\t_\t3\tadvmod\t_\t_\n",
            "\n",
            "5\tauch\tauch\tADV\tADV\t_\t3\tadvmod\t_\t_\n",
            "\n",
            "6-7\tim\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "\n",
            "6\tin\tin\tADP\tAPPR\t_\t8\tcase\t_\t_\n",
            "\n",
            "7\tdem\tder\tDET\tART\tCase=Dat|Definite=Def|Gender=Masc|Number=Sing|PronType=Art\t8\tdet\t_\t_\n",
            "\n",
            "8\tRahmen\tRahmen\tNOUN\tNN\tCase=Dat|Gender=Masc|Number=Sing\t3\tobl\t_\tSpaceAfter=No\n",
            "\n",
            "9\t.\t.\tPUNCT\t$.\t_\t3\tpunct\t_\t_\n",
            "\n",
            "\n",
            "\n",
            "# sent_id = train-s3\n",
            "\n",
            "# text = Nette Gespräche, klasse Ergebnis\n",
            "\n",
            "1\tNette\tnett\tADJ\tADJA\tCase=Nom|Gender=Neut|Number=Plur\t2\tamod\t_\t_\n",
            "\n",
            "2\tGespräche\tGespräch\tNOUN\tNN\tCase=Nom|Gender=Neut|Number=Plur\t0\troot\t_\tSpaceAfter=No\n",
            "\n",
            "3\t,\t,\tPUNCT\t$,\t_\t5\tpunct\t_\t_\n",
            "\n",
            "4\tklasse\tKlasse\tADJ\tADJA\t_\t5\tamod\t_\t_\n",
            "\n",
            "5\tErgebnis\tErgebnis\tNOUN\tNN\tCase=Nom|Gender=Neut|Number=Sing\t2\tconj\t_\t_\n",
            "\n",
            "\n",
            "\n",
            "# sent_id = train-s4\n",
            "\n",
            "# text = Ich bin seit längerer Zeit zur Behandlung verschiedenster \"Leiden\" in der Physiotherapieraxis \"Gaby Montag\" im Vital Center und kann ausschließlich Positives berichten!\n",
            "\n",
            "1\tIch\tich\tPRON\tPPER\tCase=Nom|Number=Sing|Person=1|PronType=Prs\t2\tnsubj\t_\t_\n",
            "\n",
            "2\tbin\tsein\tVERB\tVAFIN\tMood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin\t0\troot\t_\t_\n",
            "\n",
            "3\tseit\tseit\tADP\tAPPR\t_\t5\tcase\t_\t_\n",
            "\n",
            "4\tlängerer\tlang\tADJ\tADJA\tCase=Dat|Gender=Fem|Number=Sing\t5\tamod\t_\t_\n",
            "\n",
            "5\tZeit\tZeit\tNOUN\tNN\tCase=Dat|Gender=Fem|Number=Sing\t2\tobl\t_\t_\n",
            "\n",
            "6-7\tzur\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "\n",
            "6\tzu\tzu\tADP\tAPPR\t_\t8\tcase\t_\t_\n",
            "\n",
            "7\tder\tder\tDET\tART\tCase=Dat|Definite=Def|Gender=Fem|Number=Sing|PronType=Art\t8\tdet\t_\t_\n",
            "\n",
            "8\tBehandlung\tBehandlung\tNOUN\tNN\tCase=Dat|Gender=Fem|Number=Sing\t2\tobl\t_\t_\n",
            "\n",
            "9\tverschiedenster\tverschieden\tADJ\tADJA\tCase=Gen|Gender=Neut|Number=Plur\t11\tamod\t_\t_\n",
            "\n",
            "10\t\"\t\"\tPUNCT\t$(\t_\t11\tpunct\t_\tSpaceAfter=No\n",
            "\n",
            "11\tLeiden\tLeiden\tNOUN\tNN\tCase=Gen|Gender=Neut|Number=Plur\t8\tnmod\t_\tSpaceAfter=No\n",
            "\n",
            "12\t\"\t\"\tPUNCT\t$(\t_\t11\tpunct\t_\t_\n",
            "\n",
            "13\tin\tin\tADP\tAPPR\t_\t15\tcase\t_\t_\n",
            "\n",
            "14\tder\tder\tDET\tART\tCase=Dat|Definite=Def|Gender=Fem|Number=Sing|PronType=Art\t15\tdet\t_\t_\n",
            "\n",
            "15\tPhysiotherapieraxis\tPhysiotherapieraxis\tNOUN\tNN\tCase=Dat|Gender=Fem|Number=Sing\t2\tobl\t_\t_\n",
            "\n",
            "16\t\"\t\"\tPUNCT\t$(\t_\t17\tpunct\t_\tSpaceAfter=No\n",
            "\n",
            "17\tGaby\tGaby\tPROPN\tNE\tCase=Nom|Gender=Fem|Number=Sing\t15\tappos\t_\t_\n",
            "\n",
            "18\tMontag\tMontag\tPROPN\tNE\tCase=Acc|Gender=Masc|Number=Sing\t17\tflat\t_\tSpaceAfter=No\n",
            "\n",
            "19\t\"\t\"\tPUNCT\t$(\t_\t17\tpunct\t_\t_\n",
            "\n",
            "20-21\tim\t_\t_\t_\t_\t_\t_\t_\t_\n",
            "\n",
            "20\tin\tin\tADP\tAPPR\t_\t22\tcase\t_\t_\n",
            "\n",
            "21\tdem\tder\tDET\tART\tCase=Dat|Definite=Def|Gender=Neut|Number=Sing|PronType=Art\t22\tdet\t_\t_\n",
            "\n",
            "22\tVital\tVital\tPROPN\tNN\tCase=Dat|Gender=Neut|Number=Sing\t15\tnmod\t_\t_\n",
            "\n",
            "23\tCenter\tCenter\tPROPN\tNE\tCase=Dat|Gender=Neut|Number=Sing\t22\tflat\t_\t_\n",
            "\n",
            "24\tund\tund\tCCONJ\tKON\t_\t28\tcc\t_\t_\n",
            "\n",
            "25\tkann\tkönnen\tAUX\tVMFIN\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\t28\taux\t_\t_\n",
            "\n",
            "26\tausschließlich\tausschließlich\tADV\tADV\t_\t28\tadvmod\t_\t_\n",
            "\n",
            "27\tPositives\tPositive\tNOUN\tNN\tCase=Acc|Gender=Neut|Number=Sing\t28\tobj\t_\t_\n",
            "\n",
            "28\tberichten\tberichten\tVERB\tVVINF\tVerbForm=Inf\t2\tconj\t_\tSpaceAfter=No\n",
            "\n",
            "29\t!\t!\tPUNCT\t$.\t_\t2\tpunct\t_\t_\n",
            "\n",
            "\n",
            "\n",
            "# sent_id = train-s5\n",
            "\n",
            "# text = Ob bei der Terminvergabe, den Behandlungsräumen oder den individuell zugeschnittenen Trainingsplänen sind alle Mitarbeiter äußerst kompetent und flexibel.\n",
            "\n",
            "1\tOb\tob\tCCONJ\tKOUS\t_\t4\tcc\t_\t_\n",
            "\n",
            "2\tbei\tbei\tADP\tAPPR\t_\t4\tcase\t_\t_\n",
            "\n",
            "3\tder\tder\tDET\tART\tCase=Dat|Definite=Def|Gender=Fem|Number=Sing|PronType=Art\t4\tdet\t_\t_\n",
            "\n",
            "4\tTerminvergabe\tTerminvergabe\tNOUN\tNN\tCase=Dat|Gender=Fem|Number=Sing\t17\tnmod\t_\tSpaceAfter=No\n",
            "\n",
            "5\t,\t,\tPUNCT\t$,\t_\t7\tpunct\t_\t_\n",
            "\n",
            "6\tden\tder\tDET\tART\tCase=Dat|Definite=Def|Gender=Masc|Number=Plur|PronType=Art\t7\tdet\t_\t_\n",
            "\n",
            "7\tBehandlungsräumen\tBehandlungsraum\tNOUN\tNN\tCase=Dat|Gender=Masc|Number=Plur\t4\tconj\t_\t_\n",
            "\n",
            "8\toder\toder\tCCONJ\tKON\t_\t12\tcc\t_\t_\n",
            "\n",
            "9\tden\tder\tDET\tART\tCase=Dat|Definite=Def|Gender=Neut|Number=Plur|PronType=Art\t12\tdet\t_\t_\n",
            "\n",
            "10\tindividuell\tindividuell\tADV\tADJD\t_\t11\tadvmod\t_\t_\n",
            "\n",
            "11\tzugeschnittenen\tzugeschnitten\tADJ\tADJA\tCase=Dat|Gender=Masc|Number=Plur\t12\tamod\t_\t_\n",
            "\n",
            "12\tTrainingsplänen\tTrainingsplan\tNOUN\tNN\tCase=Dat|Gender=Masc|Number=Plur\t4\tconj\t_\t_\n",
            "\n",
            "13\tsind\tsein\tAUX\tVAFIN\tMood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin\t17\tcop\t_\t_\n",
            "\n",
            "14\talle\talle\tPRON\tPIAT\tCase=Nom|Definite=Ind|Gender=Masc|Number=Plur|PronType=Ind\t15\tdet\t_\t_\n",
            "\n",
            "15\tMitarbeiter\tMitarbeiter\tNOUN\tNN\tCase=Nom|Gender=Masc|Number=Plur\t17\tnsubj\t_\t_\n",
            "\n",
            "16\täußerst\täußerst\tADV\tADV\t_\t17\tadvmod\t_\t_\n",
            "\n",
            "17\tkompetent\tkompetent\tADJ\tADJD\t_\t0\troot\t_\t_\n",
            "\n",
            "18\tund\tund\tCCONJ\tKON\t_\t19\tcc\t_\t_\n",
            "\n",
            "19\tflexibel\tflexibel\tADJ\tADJD\t_\t17\tconj\t_\tSpaceAfter=No\n",
            "\n",
            "20\t.\t.\tPUNCT\t$.\t_\t17\tpunct\t_\t_\n",
            "\n",
            "\n",
            "\n",
            "# sent_id = train-s6\n",
            "\n",
            "# text = Sauberkeit, Ordnung und Freundlichkeit brauche ich hier nicht zu erwähnen, denn das gehört für mich zum Standard, der aber auch noch übertroffen wird.\n",
            "\n",
            "1\tSauberkeit\tSauberkeit\tNOUN\tNN\tCase=Nom|Gender=Fem|Number=Sing\t11\tobj\t_\tSpaceAfter=No\n",
            "\n",
            "2\t,\t,\tPUNCT\t$,\t_\t3\tpunct\t_\t_\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XcKDKVSLd1EC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd #Umstellung auf die komplexeren Tags\n",
        "ddata=dict()\n",
        "#data=dict()\n",
        "for ln, line in enumerate(open(\"UD_German-GSD/de_gsd-ud-train.conllu\")):\n",
        "  if line.startswith(\"# sent_id\"):\n",
        "    sid=line.lstrip(\"# sent_id = \").strip()\n",
        "    #data[sid]=dict()\n",
        "    #sdata=data[sid]\n",
        "  if line.startswith(\"#\"):\n",
        "    continue\n",
        "  cont=line.split(\"\\t\")\n",
        "  if len(cont)<2:\n",
        "    continue\n",
        "  if cont[3]==\"_\":\n",
        "    continue\n",
        "  #sdata[cont[0]]={\"word\":cont[1].lower(),\"tag\":cont[3]}\n",
        "  ddata[sid+\"_\"+\"{0:03}\".format(int(cont[0]))]={\"lineNumber\":int(cont[0]),\"word_l\":cont[1].lower(),\"tag\":cont[4]}\n",
        "  \n",
        "for ln, line in enumerate(open(\"UD_German-GSD/de_gsd-ud-dev.conllu\")):\n",
        "  if line.startswith(\"# sent_id\"):\n",
        "    sid=line.lstrip(\"# sent_id = \").strip()\n",
        "    #data[sid]=dict()\n",
        "    #sdata=data[sid]\n",
        "  if line.startswith(\"#\"):\n",
        "    continue\n",
        "  cont=line.split(\"\\t\")\n",
        "  if len(cont)<2:\n",
        "    continue\n",
        "  if cont[3]==\"_\":\n",
        "    continue\n",
        "  #sdata[cont[0]]={\"word\":cont[1].lower(),\"tag\":cont[3]}\n",
        "  ddata[sid+\"_\"+\"{0:03}\".format(int(cont[0]))]={\"lineNumber\":int(cont[0]),\"word_l\":cont[1].lower(),\"tag\":cont[4]}\n",
        "\n",
        "#eval=dict()\n",
        "for ln, line in enumerate(open(\"UD_German-GSD/de_gsd-ud-test.conllu\")):\n",
        "  if line.startswith(\"# sent_id\"):\n",
        "    sid=line.lstrip(\"# sent_id = \").strip()\n",
        "    #data[sid]=dict()\n",
        "    #sdata=data[sid]\n",
        "  if line.startswith(\"#\"):\n",
        "    continue\n",
        "  cont=line.split(\"\\t\")\n",
        "  if len(cont)<2:\n",
        "    continue\n",
        "  if cont[3]==\"_\":\n",
        "    continue\n",
        "  #sdata[cont[0]]={\"word\":cont[1].lower(),\"tag\":cont[3]}\n",
        "  ddata[sid+\"_\"+\"{0:03}\".format(int(cont[0]))]={\"lineNumber\":int(cont[0]),\"word_l\":cont[1].lower(),\"tag\":cont[4]}\n",
        "myframe=pd.DataFrame(ddata)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kYop51JjgWFs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uWeSVBk_hxUv",
        "colab_type": "code",
        "outputId": "6a326e9e-3a51-46f5-d4b5-efd3b2f2f121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "myframe"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>-s100_001</th>\n",
              "      <th>-s100_002</th>\n",
              "      <th>-s100_003</th>\n",
              "      <th>-s100_004</th>\n",
              "      <th>-s100_005</th>\n",
              "      <th>-s100_006</th>\n",
              "      <th>-s100_007</th>\n",
              "      <th>-s100_008</th>\n",
              "      <th>-s100_009</th>\n",
              "      <th>-s100_010</th>\n",
              "      <th>...</th>\n",
              "      <th>v-s9_006</th>\n",
              "      <th>v-s9_007</th>\n",
              "      <th>v-s9_008</th>\n",
              "      <th>v-s9_009</th>\n",
              "      <th>v-s9_010</th>\n",
              "      <th>v-s9_011</th>\n",
              "      <th>v-s9_012</th>\n",
              "      <th>v-s9_013</th>\n",
              "      <th>v-s9_014</th>\n",
              "      <th>v-s9_015</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>lineNumber</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>11</td>\n",
              "      <td>12</td>\n",
              "      <td>13</td>\n",
              "      <td>14</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tag</th>\n",
              "      <td>ART</td>\n",
              "      <td>NN</td>\n",
              "      <td>VAFIN</td>\n",
              "      <td>ADV</td>\n",
              "      <td>ADJD</td>\n",
              "      <td>NN</td>\n",
              "      <td>VAFIN</td>\n",
              "      <td>ADV</td>\n",
              "      <td>ADJD</td>\n",
              "      <td>$.</td>\n",
              "      <td>...</td>\n",
              "      <td>PPER</td>\n",
              "      <td>VAFIN</td>\n",
              "      <td>ADJD</td>\n",
              "      <td>APPR</td>\n",
              "      <td>ART</td>\n",
              "      <td>NN</td>\n",
              "      <td>KON</td>\n",
              "      <td>ART</td>\n",
              "      <td>NN</td>\n",
              "      <td>$.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>word_l</th>\n",
              "      <td>die</td>\n",
              "      <td>ausstattung</td>\n",
              "      <td>ist</td>\n",
              "      <td>sehr</td>\n",
              "      <td>spartanisch.das</td>\n",
              "      <td>zimmer</td>\n",
              "      <td>war</td>\n",
              "      <td>sehr</td>\n",
              "      <td>klein</td>\n",
              "      <td>.</td>\n",
              "      <td>...</td>\n",
              "      <td>ich</td>\n",
              "      <td>bin</td>\n",
              "      <td>begeistert</td>\n",
              "      <td>von</td>\n",
              "      <td>der</td>\n",
              "      <td>auswahl</td>\n",
              "      <td>und</td>\n",
              "      <td>der</td>\n",
              "      <td>qualität</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 292788 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           -s100_001    -s100_002 -s100_003 -s100_004        -s100_005  \\\n",
              "lineNumber         1            2         3         4                5   \n",
              "tag              ART           NN     VAFIN       ADV             ADJD   \n",
              "word_l           die  ausstattung       ist      sehr  spartanisch.das   \n",
              "\n",
              "           -s100_006 -s100_007 -s100_008 -s100_009 -s100_010   ...     \\\n",
              "lineNumber         6         7         8         9        10   ...      \n",
              "tag               NN     VAFIN       ADV      ADJD        $.   ...      \n",
              "word_l        zimmer       war      sehr     klein         .   ...      \n",
              "\n",
              "           v-s9_006 v-s9_007    v-s9_008 v-s9_009 v-s9_010 v-s9_011 v-s9_012  \\\n",
              "lineNumber        6        7           8        9       10       11       12   \n",
              "tag            PPER    VAFIN        ADJD     APPR      ART       NN      KON   \n",
              "word_l          ich      bin  begeistert      von      der  auswahl      und   \n",
              "\n",
              "           v-s9_013  v-s9_014 v-s9_015  \n",
              "lineNumber       13        14       15  \n",
              "tag             ART        NN       $.  \n",
              "word_l          der  qualität        .  \n",
              "\n",
              "[3 rows x 292788 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "5jZcDWpeljck",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Cs_so9pnsJ6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Gut, wandeln wir das mal in Sequenzen von Sätzen um:\n",
        "target=[]\n",
        "train=[]\n",
        "md=[]\n",
        "mt=[]\n",
        "\n",
        "for ln in myframe:\n",
        "  mo=myframe[ln]\n",
        "  if mo[\"lineNumber\"]==1:\n",
        "    if md:\n",
        "      target.append(mt)\n",
        "      train.append(md)\n",
        "    md=[]\n",
        "    mt=[]\n",
        "  md.append(mo[\"word_l\"])\n",
        "  mt.append(mo[\"tag\"])\n",
        "md.append(mo[\"word_l\"])\n",
        "mt.append(mo[\"tag\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SXWGg7nhEpi2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"etarget=[]\n",
        "etrain=[]\n",
        "md=[]\n",
        "mt=[]\n",
        "\n",
        "for ln in evaluation:\n",
        "  mo=evaluation[ln]\n",
        "  if mo[\"lineNumber\"]==1:\n",
        "    if md:\n",
        "      etarget.append(mt)\n",
        "      etrain.append(md)\n",
        "    md=[]\n",
        "    mt=[]\n",
        "  md.append(mo[\"word_l\"])\n",
        "  mt.append(mo[\"tag\"])\n",
        "etarget.append(mt)\n",
        "etrain.append(md)\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "be0Z8wMZrBf_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#del(myframe)\n",
        "\n",
        "#del(evaluation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rK-nN7XSp2ML",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#myframe[ln]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ib9KyHvOtA46",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "outputId": "5d57d9ab-caa6-4ac4-afc7-97dc11d6a584"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Das hier sind Schritte, um das Modell so klein zu machen, dass es auch in den Arbeitsspeicher von Colab passt.\n",
        "Dafür lade ich es, und schreibe jedes enthaltene Wort in einen neuen Vektor\n",
        "Im nächsten durchlauf können wir dann die Embedding Matrix aus den Savedvecs erstellen\n",
        "\n",
        "Ist das hier fertig, kann man die Runtime neu starten und das erstellte Modell einlesen.\n",
        "Das Problem ist sonst, dass zu viel Speicher allokiert wird (warum auch immer!)\n",
        "\n",
        "\"\"\"\n",
        "if 1==1:\n",
        "  !pip install gensim\n",
        "  from gensim.models.keyedvectors import KeyedVectors \n",
        "\n",
        "  wordset={word for sentence in train for word in sentence}\n",
        "  !wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/word-vectors-v2/cc.de.300.vec.gz\n",
        "  model=KeyedVectors.load_word2vec_format(\"cc.de.300.vec.gz\",binary=False)\n",
        "  !rm cc.de.300.* \n",
        "  mymodel=KeyedVectors(300)\n",
        "  for word in wordset:\n",
        "    if word in model:\n",
        "      mymodel.add(word, model[word])\n",
        "  del(model)\n",
        "  mymodel.save(\"Savedvecs.gz\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/a4/d10c0acc8528d838cda5eede0ee9c784caa598dbf40bd0911ff8d067a7eb/gensim-3.6.0-cp36-cp36m-manylinux1_x86_64.whl (23.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 23.6MB 1.3MB/s \n",
            "\u001b[?25hCollecting smart-open>=1.2.1 (from gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/1f/6f27e3682124de63ac97a0a5876da6186de6c19410feab66c1543afab055/smart_open-1.7.1.tar.gz\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 15.2MB/s \n",
            "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/e9/b61793a571f9b5840d5a5885e4fb46ab13cb1aae0c465a4ccee67d2e677b/boto3-1.9.61-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 27.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.11.29)\n",
            "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
            "Collecting botocore<1.13.0,>=1.12.61 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/c2/f6f8c5adafef5665d0aaab86347ff7fc9b580ba8382a4e48a9beaeb20f7e/botocore-1.12.61-py2.py3-none-any.whl (5.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.1MB 6.5MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 23.1MB/s \n",
            "\u001b[?25hCollecting docutils>=0.10 (from botocore<1.13.0,>=1.12.61->boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 24.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.61->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Building wheels for collected packages: smart-open, bz2file\n",
            "  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/23/00/44/e5b939f7a80c04e32297dbd6d96fa3065af89ecf57e2b5f89f\n",
            "  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "Successfully built smart-open bz2file\n",
            "Installing collected packages: boto, bz2file, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
            "Successfully installed boto-2.49.0 boto3-1.9.61 botocore-1.12.61 bz2file-0.98 docutils-0.14 gensim-3.6.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.7.1\n",
            "--2018-12-06 22:12:28--  https://s3-us-west-1.amazonaws.com/fasttext-vectors/word-vectors-v2/cc.de.300.vec.gz\n",
            "Resolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 52.219.20.89\n",
            "Connecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|52.219.20.89|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1278030050 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.de.300.vec.gz’\n",
            "\n",
            "cc.de.300.vec.gz    100%[===================>]   1.19G  22.1MB/s    in 56s     \n",
            "\n",
            "2018-12-06 22:13:25 (21.6 MB/s) - ‘cc.de.300.vec.gz’ saved [1278030050/1278030050]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Uz5tWcDtDg0i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "9000b985-9308-4a7c-bcaf-1f6220d33f79"
      },
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.7.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.58)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.10.15)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.58 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.58)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.58->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.58->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A-qwtxd1tutl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XbE8GzJLt3D-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eY68CguSz5uf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PUeYnNhFuq-q",
        "colab_type": "code",
        "outputId": "5646540a-2c2d-4496-ff45-c30a8634989c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-d28f8bda2dc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmymodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wordset' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "l_GSAcCPu68J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#mymodel.save(\"Savedvecs.gz\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oHns0xj1zmmj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WxiApBv7oBU_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#line"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nz8LdUI5tmyr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "KDW"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sJgr4tYCmkaN",
        "colab_type": "code",
        "outputId": "fb438801-41ae-408b-b735-96483df9e673",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import keras"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "tGHbxP1RlnBN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer=keras.preprocessing.text.Tokenizer(filters=\"\") #NEU\n",
        "tokenizer.fit_on_texts(train)\n",
        "X1=tokenizer.texts_to_sequences(train)\n",
        "#EX=tokenizer.texts_to_sequences(etrain)\n",
        "#del(train)\n",
        "#del(etrain)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fsL94d3s6NiQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seqlength=max([len(item) for item in X1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rg-_Nwmv6Y1s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH=40\n",
        "X1=keras.preprocessing.sequence.pad_sequences(X1, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\",truncating= 'post')\n",
        "#EX=keras.preprocessing.sequence.pad_sequences(EX, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\",truncating= 'post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eQs3zYq_XPrV",
        "colab_type": "code",
        "outputId": "c31177c5-ddbb-49b2-c89f-3972106ad954",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "X1.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15589, 40)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "H8aivAfMSEX6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CHARSEQLEN=10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LzzhLeJhSEUm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_baskets=[list() for i in range(MAX_SEQUENCE_LENGTH)]\n",
        "#AuxilaryInputs\n",
        "for text in train:\n",
        "  for i in range(MAX_SEQUENCE_LENGTH):\n",
        "    if len(text)<=i:\n",
        "      input_baskets[i].append(\"_\")\n",
        "    else:\n",
        "      input_baskets[i].append(text[i])\n",
        "#Nun sollten alle baskets gleich lang sein"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OsB2y-YwUrMh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "outputId": "16392833-faa5-41d9-a831-339692448e38"
      },
      "cell_type": "code",
      "source": [
        "for basket in input_baskets:\n",
        "  print(len(basket))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n",
            "15589\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ts0T8FQeXliX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "702c534b-ae80-4b48-b808-ece12f94289f"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'_'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "p2ib-BwHSER2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "aux_tokenizer=keras.preprocessing.text.Tokenizer(char_level=True,filters=\"\") #Hier brauchen wir die buchstabeneinstellung\n",
        "aux_tokenizer.fit_on_texts(train)\n",
        "auxseq=[pad_sequences(aux_tokenizer.texts_to_sequences(basket), maxlen=CHARSEQLEN, padding=\"pre\",truncating='pre') for basket in input_baskets]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JRMczprWSEO7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2b89abca-2140-4b78-f065-c328078fc2c5"
      },
      "cell_type": "code",
      "source": [
        "auxseq[2][2]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0, 3444, 4954, 6730,  290,  832], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "1GapYZrySEHC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "efd8a83b-dd03-4593-b509-7a8c0952cbab"
      },
      "cell_type": "code",
      "source": [
        "len(aux_tokenizer.word_index)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50490"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "57yd_fp9d0nb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g20eWXNbx7BH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "targetTokenizer=keras.preprocessing.text.Tokenizer()\n",
        "targetTokenizer.fit_on_texts(target)\n",
        "Y=targetTokenizer.texts_to_sequences(target)\n",
        "#EY=targetTokenizer.texts_to_sequences(etarget)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bKH6EFG3m_aN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#ctarget=targetTokenizer.texts_to_sequences(target+etarget)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oTblrpbknpoL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Y=ctarget[:len(target)]\n",
        "#EY=ctarget[len(target):]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "66Eb_JqOn2tb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6eiTIkwzeVCn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "outputId": "b36d9d49-aa4d-4313-a9e1-605369e08d4c"
      },
      "cell_type": "code",
      "source": [
        "Y.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-de29cf719ad1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "OOhQDa4QAD1N",
        "colab_type": "code",
        "outputId": "6f6103a4-8f52-4b52-873d-c3a4d11af57d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "numoftags=len({pos for sentence in target for pos in sentence})\n",
        "print(numoftags)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-ziGDMVNnOaC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a7ae5f8e-3ee8-4da7-da32-f009cb787f40"
      },
      "cell_type": "code",
      "source": [
        "#print(len({pos for sentence in etarget for pos in sentence}))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_tj47HWPewJv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Y=np.array([[i==num for i in range(numoftags)] for sentence in Y for num in sentence],dtype=np.bool)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UfICJRYrfPM0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#EY=np.array([[i==num for i in range(numoftags)] for sentence in EY for num in sentence],dtype=np.bool)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tUk9KoJ07Q1b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Y=keras.preprocessing.sequence.pad_sequences(Y, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\",truncating= 'post')\n",
        "#EY=keras.preprocessing.sequence.pad_sequences(EY, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\",truncating= 'post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lsfq6jAehFw8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4bNV8Twof-b9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.bool"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vc3PqImkeDGd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BMxZtqMqjbPt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Y=to_categorical(Y, num_classes=53)\n",
        "#EY=to_categorical(EY, num_classes=17)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ryMTvumK7Vj-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c6a88047-bc32-41d2-dceb-de4742c92e18"
      },
      "cell_type": "code",
      "source": [
        "Y.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15589, 40, 53)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "n0_kwoJ-hNB5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SmErfuLfdVXO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EY.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dbx9Et4tyg6r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A--HWRtFy703",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1vzvgsfmzyCT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "#import gensim.models.keyedvectors as kv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wnZfrHXXoeJQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!rm cc.de.300.*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "orXQo3Gdz_vu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#model=KeyedVectors.load_word2vec_format(\"cc.de.300.vec.gz\",binary=False)\n",
        "#model=KeyedVectors.load_word2vec_format(\"https://s3-us-west-1.amazonaws.com/fasttext-vectors/word-vectors-v2/cc.de.300.vec.gz\",binary=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3atcP_bHEK28",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#del(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZqL8NSkqKULU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weights=np.random.random((5,30))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JF3htyf4KWTQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "354d142c-e72e-4dde-c059-c6547d75aa9e"
      },
      "cell_type": "code",
      "source": [
        "model"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.training.Model at 0x7fd18a4a33c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "Qe8Sg7_Fp-Yr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors \n",
        "gmodel=KeyedVectors.load(\"Savedvecs.gz\")\n",
        "weights=np.random.random((len(tokenizer.word_index)+1,300))\n",
        "for token in tokenizer.word_index:\n",
        "  if token in gmodel:\n",
        "    weights[tokenizer.word_index[token]]=gmodel[token]\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nna1wYyRaTPq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2d668df5-13d9-4be1-ae0a-491595c75fc8"
      },
      "cell_type": "code",
      "source": [
        "weights.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50491, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "Jp_zIkUeKkRY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "d0bd20bc-70d9-4651-987c-82aa0ec13ce8"
      },
      "cell_type": "code",
      "source": [
        "weights"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.71360366,  0.89183436,  0.08849387, ...,  0.86731829,\n",
              "         0.25897333,  0.83273826],\n",
              "       [-0.0257    ,  0.0093    , -0.0562    , ...,  0.0404    ,\n",
              "        -0.0495    , -0.0133    ],\n",
              "       [-0.0354    , -0.0088    , -0.018     , ..., -0.0606    ,\n",
              "        -0.1096    ,  0.0739    ],\n",
              "       ...,\n",
              "       [ 0.18851948,  0.51263812,  0.63704004, ...,  0.64186789,\n",
              "         0.25307476,  0.23443716],\n",
              "       [ 0.84113416,  0.54553801,  0.0655744 , ...,  0.71205351,\n",
              "         0.97951319,  0.68883236],\n",
              "       [ 0.0064    , -0.0098    ,  0.0024    , ...,  0.01      ,\n",
              "        -0.0088    ,  0.0015    ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "6KNfmKBE0oxW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pMKyQHcDqHN9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#embedding=model.get_keras_embedding()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KHvXAw8eBJrY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gDz8yHWaPwg8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D, Flatten\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, TimeDistributed, Bidirectional\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "#embedded_sequences = embedding(sequence_input)\n",
        "embedded_sequences=Embedding(50490,300, weights=[weights],mask_zero=True)(sequence_input)\n",
        "x = Bidirectional(LSTM(100,return_sequences=True))(embedded_sequences)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Bidirectional(LSTM(128,return_sequences=True))(x)\n",
        "embed2= Embedding(50490, output_dim=700,mask_zero=True)(sequence_input)\n",
        "x2= Bidirectional(LSTM(128,return_sequences=True))(embed2)\n",
        "x2 = Dropout(0.5)(x2)\n",
        "x2= Bidirectional(LSTM(128,return_sequences=True))(x2)\n",
        "x = keras.layers.concatenate([x, x2])\n",
        "x = Dropout(0.5)(x)\n",
        "x = Bidirectional(LSTM(128,return_sequences=True))(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Bidirectional(LSTM(64,return_sequences=True))(x)\n",
        "preds = TimeDistributed(Dense(numoftags+1,activation=\"softmax\"))(x)\n",
        "\n",
        "model = Model(sequence_input, preds)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
        "model.fit(X1, Y, epochs=100, batch_size=256, validation_split=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UendiMKM54V4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"from keras.models import Sequential\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D, Flatten\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, TimeDistributed, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input(dtype='int32',tensor=X1))\n",
        "model.add(embedding)\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(300, 3, padding='valid',activation='relu',strides=2))\n",
        "model.add(Conv1D(150, 3, padding='valid',activation='relu',strides=2))\n",
        "model.add(Conv1D(75, 3, padding='valid',activation='relu',strides=2))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(TimeDistributed(Dense(150,activation='sigmoid')))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(TimeDistributed(Dense(numoftags,activation='sigmoid')))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
        "\n",
        "model.summary()\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RuzlhviHHJuv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X1.max()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-FtRylu6BCKw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#keras.layers.TimeDistributed()\n",
        "model_1 = Sequential()\n",
        "model_1.add(Input((50,)))\n",
        "model_1.add(embedding(X1))\n",
        "model_1.add(Bidirectional(LSTM(256,return_sequences=True)))\n",
        "model_1.add(Bidirectional(LSTM(256,return_sequences=True)))\n",
        "model_1.add(TimeDistributed(Dense(250)))\n",
        "model_1.add(Dropout(0.2))\n",
        "model_1.add(Activation('relu'))\n",
        "model_1.add(TimeDistributed(Dense(numoftags+1,activation=\"softmax\")))\n",
        "#model_1.add(Activation('sigmoid'))\n",
        "model_1.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
        "model_1.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bcyFohx_CY_D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model.fit(x_train, y_train)#, validation_data=(x_val, y_val), epochs=2, batch_size=128)\n",
        "score = model.evaluate(x_val, y_val, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zgl5zpAyCkDB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_1.fit(X1, Y, validation_data=(EX, EY), epochs=100, batch_size=128)\n",
        "score = model_1.evaluate(EX, EY, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jgnB6ofrWX5-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#for slice in Y:\n",
        " ä print(slice.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6u4-BPUz0N7a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VrOTUnjmr6Ga",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K-lih8hOrfOx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X1=tokenizer.sequences_to_matrix(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yp2cSHcymHj8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Verwenden wir lieber mal testweise ein BiLSTM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bXWr2Vb6gaDm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1QsVxDAaf_51",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2447
        },
        "outputId": "f077747f-c9ed-4ef3-d950-89a08b4968de"
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_23 (InputLayer)           (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_24 (InputLayer)           (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_25 (InputLayer)           (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_26 (InputLayer)           (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_27 (InputLayer)           (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_28 (InputLayer)           (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_29 (InputLayer)           (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_30 (InputLayer)           (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_31 (InputLayer)           (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_32 (InputLayer)           (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_33 (InputLayer)           (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_34 (InputLayer)           (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_35 (InputLayer)           (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_36 (InputLayer)           (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_37 (InputLayer)           (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_38 (InputLayer)           (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_39 (InputLayer)           (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_40 (InputLayer)           (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_41 (InputLayer)           (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_42 (InputLayer)           (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_43 (InputLayer)           (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, 20, 300)      15147300    input_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 20, 30)       1514700     input_24[0][0]                   \n",
            "                                                                 input_25[0][0]                   \n",
            "                                                                 input_26[0][0]                   \n",
            "                                                                 input_27[0][0]                   \n",
            "                                                                 input_28[0][0]                   \n",
            "                                                                 input_29[0][0]                   \n",
            "                                                                 input_30[0][0]                   \n",
            "                                                                 input_31[0][0]                   \n",
            "                                                                 input_32[0][0]                   \n",
            "                                                                 input_33[0][0]                   \n",
            "                                                                 input_34[0][0]                   \n",
            "                                                                 input_35[0][0]                   \n",
            "                                                                 input_36[0][0]                   \n",
            "                                                                 input_37[0][0]                   \n",
            "                                                                 input_38[0][0]                   \n",
            "                                                                 input_39[0][0]                   \n",
            "                                                                 input_40[0][0]                   \n",
            "                                                                 input_41[0][0]                   \n",
            "                                                                 input_42[0][0]                   \n",
            "                                                                 input_43[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_13 (Bidirectional (None, 20, 40)       51360       embedding_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_15 (Bidirectional (None, 20, 40)       8160        embedding_6[0][0]                \n",
            "                                                                 embedding_6[1][0]                \n",
            "                                                                 embedding_6[2][0]                \n",
            "                                                                 embedding_6[3][0]                \n",
            "                                                                 embedding_6[4][0]                \n",
            "                                                                 embedding_6[5][0]                \n",
            "                                                                 embedding_6[6][0]                \n",
            "                                                                 embedding_6[7][0]                \n",
            "                                                                 embedding_6[8][0]                \n",
            "                                                                 embedding_6[9][0]                \n",
            "                                                                 embedding_6[10][0]               \n",
            "                                                                 embedding_6[11][0]               \n",
            "                                                                 embedding_6[12][0]               \n",
            "                                                                 embedding_6[13][0]               \n",
            "                                                                 embedding_6[14][0]               \n",
            "                                                                 embedding_6[15][0]               \n",
            "                                                                 embedding_6[16][0]               \n",
            "                                                                 embedding_6[17][0]               \n",
            "                                                                 embedding_6[18][0]               \n",
            "                                                                 embedding_6[19][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 20, 40)       0           bidirectional_13[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 20, 800)      0           bidirectional_15[0][0]           \n",
            "                                                                 bidirectional_15[1][0]           \n",
            "                                                                 bidirectional_15[2][0]           \n",
            "                                                                 bidirectional_15[3][0]           \n",
            "                                                                 bidirectional_15[4][0]           \n",
            "                                                                 bidirectional_15[5][0]           \n",
            "                                                                 bidirectional_15[6][0]           \n",
            "                                                                 bidirectional_15[7][0]           \n",
            "                                                                 bidirectional_15[8][0]           \n",
            "                                                                 bidirectional_15[9][0]           \n",
            "                                                                 bidirectional_15[10][0]          \n",
            "                                                                 bidirectional_15[11][0]          \n",
            "                                                                 bidirectional_15[12][0]          \n",
            "                                                                 bidirectional_15[13][0]          \n",
            "                                                                 bidirectional_15[14][0]          \n",
            "                                                                 bidirectional_15[15][0]          \n",
            "                                                                 bidirectional_15[16][0]          \n",
            "                                                                 bidirectional_15[17][0]          \n",
            "                                                                 bidirectional_15[18][0]          \n",
            "                                                                 bidirectional_15[19][0]          \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_14 (Bidirectional (None, 20, 40)       9760        dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_16 (Bidirectional (None, 20, 256)      951296      concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 20, 296)      0           bidirectional_14[0][0]           \n",
            "                                                                 bidirectional_16[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 20, 296)      0           concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_17 (Bidirectional (None, 20, 256)      435200      dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 20, 256)      0           bidirectional_17[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_18 (Bidirectional (None, 20, 128)      164352      dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_3 (TimeDistrib (None, 20, 17)       2193        bidirectional_18[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 18,284,321\n",
            "Trainable params: 18,284,321\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b7cC35qLf3_n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#AUXMODEL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YUW7oLM4RRJq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#del(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qyDwi-V1S20x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import RepeatVector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mVLc-jMJfyyt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "55d90505-db90-4157-ef5f-dc3186164f93"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D, Flatten\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, TimeDistributed, Bidirectional\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences=Embedding(50491,300, weights=[weights],mask_zero=True)(sequence_input)\n",
        "x = Bidirectional(LSTM(100,return_sequences=True))(embedded_sequences)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Bidirectional(LSTM(100,return_sequences=True))(x) # ich habe die beiden mal deutlich reduziert\n",
        "\n",
        "aux_inputs=[Input(shape=(CHARSEQLEN,), dtype=\"int32\") for i in range(20)]\n",
        "aux_embedding=Embedding(len(aux_tokenizer.word_index),50,mask_zero=True)\n",
        "aux_embedded=[aux_embedding(inp) for inp in aux_inputs]\n",
        "aux_lstm=LSTM(100)#,return_sequences=True)\n",
        "aux_x = [Dropout(0.3)(aux_lstm(vals)) for vals in aux_embedded]\n",
        "x2=keras.layers.concatenate(aux_x)\n",
        "x2=Dense(4000)(x2)\n",
        "x2=Dense(300)(x2)\n",
        "x2=RepeatVector(20)(x2)#noch in variable ändern\n",
        "#x2=Bidirectional(LSTM(128,return_sequences=True))(x2)\n",
        "x = keras.layers.concatenate([x, x2])\n",
        "#x=x2\n",
        "x = Dropout(0.5)(x)\n",
        "x = Bidirectional(LSTM(256,return_sequences=True))(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Bidirectional(LSTM(128,return_sequences=True))(x)\n",
        "preds = TimeDistributed(Dense(numoftags+1,activation=\"softmax\"))(x)\n",
        "\n",
        "model = Model([sequence_input]+aux_inputs, preds)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
        "model.fit([X1]+auxseq, Y, epochs=100, batch_size=128, validation_split=0.1)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-4fe32f4eb7ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msequence_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0maux_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mauxseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected time_distributed_2 to have 3 dimensions, but got array with shape (15589, 20, 17, 17)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "xQHQDv2udXqm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d9ab862f-cb6a-46cc-bdc4-a564898a25b4"
      },
      "cell_type": "code",
      "source": [
        "[1]+[2,3entstehen]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "hHx73yH6dZbC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Wir implementieren in der Folge 2 Architekturen:\n",
        "\"\"\"\n",
        " Die 50dim 20LSTMs beenden ihre Sequenzen und kippen das Ergebnis in einen großen Vektor\n",
        "1. Nun verwenden wir den konkatenierten Vektor,\n",
        "2.\n",
        "\n",
        "\"\"\"\n",
        "#PS: Wenn das da oben konvergiert, bastle ich ein Teilmodell, das ich dann in einen Vektor stecke, den ich dann einfriere\n",
        "#Dieser Vektor wird dann in einem weiteren Modell als zusätzlicher Layer verwendet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P5OIMOQUkLIb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Reshape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PBe3mKUoXsPY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1452
        },
        "outputId": "53b6bd80-31b6-403c-ce41-6fc9f2bf2a95"
      },
      "cell_type": "code",
      "source": [
        "#Version 2\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D, Flatten\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, TimeDistributed, Bidirectional, CuDNNLSTM\n",
        "from keras.layers.merge import add\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences=Embedding(50491,300, weights=[weights])(sequence_input)\n",
        "x = Dropout(0.75)(embedded_sequences)\n",
        "xn = Bidirectional(CuDNNLSTM(128,return_sequences=True))(x)\n",
        "x = Dropout(0.75)(xn)\n",
        "x = Bidirectional(CuDNNLSTM(100,return_sequences=True))(x) # ich habe die beiden mal deutlich reduziert\n",
        "\n",
        "aux_inputs=[Input(shape=(CHARSEQLEN,), dtype=\"int32\") for i in range(MAX_SEQUENCE_LENGTH)]\n",
        "aux_embedding=Embedding(len(aux_tokenizer.word_index),30)\n",
        "aux_embedded=[aux_embedding(inp) for inp in aux_inputs]\n",
        "aux_lstm=CuDNNLSTM(200)#,return_sequences=True)\n",
        "aux_x = [Dropout(0.3)(aux_lstm(vals)) for vals in aux_embedded]\n",
        "dense=Dense(200)\n",
        "aux_x = [dense(vals) for vals in aux_x]\n",
        "x2=keras.layers.concatenate(aux_x) #Über welche Achse machen wir das denn?\n",
        "x2=Reshape((MAX_SEQUENCE_LENGTH,200))(x2)\n",
        "x2=Dropout(0.4)(x2)\n",
        "x2=CuDNNLSTM(200, return_sequences=True)(x2)\n",
        "x2=Bidirectional(CuDNNLSTM(200,return_sequences=True))(x2)\n",
        "x = keras.layers.concatenate([x, x2])\n",
        "#x=x2\n",
        "x = Dropout(0.5)(x)\n",
        "x = Bidirectional(CuDNNLSTM(256,return_sequences=True))(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Bidirectional(CuDNNLSTM(128,return_sequences=True))(x)\n",
        "x = add([x,xn])\n",
        "preds = TimeDistributed(Dense(numoftags+1,activation=\"softmax\"))(x)\n",
        "\n",
        "model = Model([sequence_input]+aux_inputs, preds)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
        "hist=model.fit([X1]+auxseq, Y, epochs=100, batch_size=128, validation_split=0.1)\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 14030 samples, validate on 1559 samples\n",
            "Epoch 1/100\n",
            "14030/14030 [==============================] - 60s 4ms/step - loss: 1.1316 - acc: 0.6505 - val_loss: 0.4452 - val_acc: 0.8726\n",
            "Epoch 2/100\n",
            "14030/14030 [==============================] - 30s 2ms/step - loss: 0.3483 - acc: 0.8919 - val_loss: 0.2242 - val_acc: 0.9282\n",
            "Epoch 3/100\n",
            "14030/14030 [==============================] - 30s 2ms/step - loss: 0.2347 - acc: 0.9249 - val_loss: 0.1916 - val_acc: 0.9400\n",
            "Epoch 4/100\n",
            "14030/14030 [==============================] - 30s 2ms/step - loss: 0.1981 - acc: 0.9368 - val_loss: 0.1710 - val_acc: 0.9446\n",
            "Epoch 5/100\n",
            "14030/14030 [==============================] - 30s 2ms/step - loss: 0.1761 - acc: 0.9437 - val_loss: 0.1633 - val_acc: 0.9477\n",
            "Epoch 6/100\n",
            "14030/14030 [==============================] - 30s 2ms/step - loss: 0.1603 - acc: 0.9490 - val_loss: 0.1539 - val_acc: 0.9518\n",
            "Epoch 7/100\n",
            "14030/14030 [==============================] - 30s 2ms/step - loss: 0.1482 - acc: 0.9529 - val_loss: 0.1498 - val_acc: 0.9533\n",
            "Epoch 8/100\n",
            "14030/14030 [==============================] - 30s 2ms/step - loss: 0.1380 - acc: 0.9561 - val_loss: 0.1425 - val_acc: 0.9546\n",
            "Epoch 9/100\n",
            "14030/14030 [==============================] - 30s 2ms/step - loss: 0.1299 - acc: 0.9587 - val_loss: 0.1392 - val_acc: 0.9564\n",
            "Epoch 10/100\n",
            "14030/14030 [==============================] - 30s 2ms/step - loss: 0.1229 - acc: 0.9606 - val_loss: 0.1409 - val_acc: 0.9556\n",
            "Epoch 11/100\n",
            "14030/14030 [==============================] - 30s 2ms/step - loss: 0.1168 - acc: 0.9622 - val_loss: 0.1380 - val_acc: 0.9563\n",
            "Epoch 12/100\n",
            "14030/14030 [==============================] - 30s 2ms/step - loss: 0.1101 - acc: 0.9645 - val_loss: 0.1367 - val_acc: 0.9569\n",
            "Epoch 13/100\n",
            "14030/14030 [==============================] - 30s 2ms/step - loss: 0.1040 - acc: 0.9660 - val_loss: 0.1361 - val_acc: 0.9569\n",
            "Epoch 14/100\n",
            "14030/14030 [==============================] - 30s 2ms/step - loss: 0.0995 - acc: 0.9676 - val_loss: 0.1414 - val_acc: 0.9550\n",
            "Epoch 15/100\n",
            "14030/14030 [==============================] - 30s 2ms/step - loss: 0.0946 - acc: 0.9692 - val_loss: 0.1378 - val_acc: 0.9568\n",
            "Epoch 16/100\n",
            "  128/14030 [..............................] - ETA: 28s - loss: 0.0992 - acc: 0.9652"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-f4b8b4d034ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msequence_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0maux_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mauxseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cloHcX1ogco0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "model.add(Conv1D(300, 3, padding='valid',activation='relu',strides=2))\n",
        "model.add(Conv1D(150, 3, padding='valid',activation='relu',strides=2))\n",
        "model.add(Conv1D(75, 3, padding='valid',activation='relu',strides=2))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X1T6omByna9A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Kombinieren wir das mal mit einem traditionellen Tagger? Die Wortfeatures sind schon mal interessant, aber mal abwarten, ob man sie noch optimieren kann!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jbpCxUocihNL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3743
        },
        "outputId": "0e499461-1360-4f7e-8fe3-307e60fec444"
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_195 (InputLayer)          (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_196 (InputLayer)          (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_197 (InputLayer)          (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_198 (InputLayer)          (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_199 (InputLayer)          (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_200 (InputLayer)          (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_201 (InputLayer)          (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_202 (InputLayer)          (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_203 (InputLayer)          (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_204 (InputLayer)          (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_205 (InputLayer)          (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_206 (InputLayer)          (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_207 (InputLayer)          (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_208 (InputLayer)          (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_209 (InputLayer)          (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_210 (InputLayer)          (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_211 (InputLayer)          (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_212 (InputLayer)          (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_213 (InputLayer)          (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_214 (InputLayer)          (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_24 (Embedding)        (None, 20, 30)       1514700     input_195[0][0]                  \n",
            "                                                                 input_196[0][0]                  \n",
            "                                                                 input_197[0][0]                  \n",
            "                                                                 input_198[0][0]                  \n",
            "                                                                 input_199[0][0]                  \n",
            "                                                                 input_200[0][0]                  \n",
            "                                                                 input_201[0][0]                  \n",
            "                                                                 input_202[0][0]                  \n",
            "                                                                 input_203[0][0]                  \n",
            "                                                                 input_204[0][0]                  \n",
            "                                                                 input_205[0][0]                  \n",
            "                                                                 input_206[0][0]                  \n",
            "                                                                 input_207[0][0]                  \n",
            "                                                                 input_208[0][0]                  \n",
            "                                                                 input_209[0][0]                  \n",
            "                                                                 input_210[0][0]                  \n",
            "                                                                 input_211[0][0]                  \n",
            "                                                                 input_212[0][0]                  \n",
            "                                                                 input_213[0][0]                  \n",
            "                                                                 input_214[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "cu_dnnlstm_7 (CuDNNLSTM)        (None, 200)          185600      embedding_24[0][0]               \n",
            "                                                                 embedding_24[1][0]               \n",
            "                                                                 embedding_24[2][0]               \n",
            "                                                                 embedding_24[3][0]               \n",
            "                                                                 embedding_24[4][0]               \n",
            "                                                                 embedding_24[5][0]               \n",
            "                                                                 embedding_24[6][0]               \n",
            "                                                                 embedding_24[7][0]               \n",
            "                                                                 embedding_24[8][0]               \n",
            "                                                                 embedding_24[9][0]               \n",
            "                                                                 embedding_24[10][0]              \n",
            "                                                                 embedding_24[11][0]              \n",
            "                                                                 embedding_24[12][0]              \n",
            "                                                                 embedding_24[13][0]              \n",
            "                                                                 embedding_24[14][0]              \n",
            "                                                                 embedding_24[15][0]              \n",
            "                                                                 embedding_24[16][0]              \n",
            "                                                                 embedding_24[17][0]              \n",
            "                                                                 embedding_24[18][0]              \n",
            "                                                                 embedding_24[19][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_200 (Dropout)           (None, 200)          0           cu_dnnlstm_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_201 (Dropout)           (None, 200)          0           cu_dnnlstm_7[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_202 (Dropout)           (None, 200)          0           cu_dnnlstm_7[2][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_203 (Dropout)           (None, 200)          0           cu_dnnlstm_7[3][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_204 (Dropout)           (None, 200)          0           cu_dnnlstm_7[4][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_205 (Dropout)           (None, 200)          0           cu_dnnlstm_7[5][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_206 (Dropout)           (None, 200)          0           cu_dnnlstm_7[6][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_207 (Dropout)           (None, 200)          0           cu_dnnlstm_7[7][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_208 (Dropout)           (None, 200)          0           cu_dnnlstm_7[8][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_209 (Dropout)           (None, 200)          0           cu_dnnlstm_7[9][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_210 (Dropout)           (None, 200)          0           cu_dnnlstm_7[10][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_211 (Dropout)           (None, 200)          0           cu_dnnlstm_7[11][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_212 (Dropout)           (None, 200)          0           cu_dnnlstm_7[12][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_213 (Dropout)           (None, 200)          0           cu_dnnlstm_7[13][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_214 (Dropout)           (None, 200)          0           cu_dnnlstm_7[14][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_215 (Dropout)           (None, 200)          0           cu_dnnlstm_7[15][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_216 (Dropout)           (None, 200)          0           cu_dnnlstm_7[16][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_217 (Dropout)           (None, 200)          0           cu_dnnlstm_7[17][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_218 (Dropout)           (None, 200)          0           cu_dnnlstm_7[18][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_219 (Dropout)           (None, 200)          0           cu_dnnlstm_7[19][0]              \n",
            "__________________________________________________________________________________________________\n",
            "input_194 (InputLayer)          (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_74 (Dense)                (None, 200)          40200       dropout_200[0][0]                \n",
            "                                                                 dropout_201[0][0]                \n",
            "                                                                 dropout_202[0][0]                \n",
            "                                                                 dropout_203[0][0]                \n",
            "                                                                 dropout_204[0][0]                \n",
            "                                                                 dropout_205[0][0]                \n",
            "                                                                 dropout_206[0][0]                \n",
            "                                                                 dropout_207[0][0]                \n",
            "                                                                 dropout_208[0][0]                \n",
            "                                                                 dropout_209[0][0]                \n",
            "                                                                 dropout_210[0][0]                \n",
            "                                                                 dropout_211[0][0]                \n",
            "                                                                 dropout_212[0][0]                \n",
            "                                                                 dropout_213[0][0]                \n",
            "                                                                 dropout_214[0][0]                \n",
            "                                                                 dropout_215[0][0]                \n",
            "                                                                 dropout_216[0][0]                \n",
            "                                                                 dropout_217[0][0]                \n",
            "                                                                 dropout_218[0][0]                \n",
            "                                                                 dropout_219[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "embedding_23 (Embedding)        (None, 20, 300)      15147300    input_194[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 4000)         0           dense_74[0][0]                   \n",
            "                                                                 dense_74[1][0]                   \n",
            "                                                                 dense_74[2][0]                   \n",
            "                                                                 dense_74[3][0]                   \n",
            "                                                                 dense_74[4][0]                   \n",
            "                                                                 dense_74[5][0]                   \n",
            "                                                                 dense_74[6][0]                   \n",
            "                                                                 dense_74[7][0]                   \n",
            "                                                                 dense_74[8][0]                   \n",
            "                                                                 dense_74[9][0]                   \n",
            "                                                                 dense_74[10][0]                  \n",
            "                                                                 dense_74[11][0]                  \n",
            "                                                                 dense_74[12][0]                  \n",
            "                                                                 dense_74[13][0]                  \n",
            "                                                                 dense_74[14][0]                  \n",
            "                                                                 dense_74[15][0]                  \n",
            "                                                                 dense_74[16][0]                  \n",
            "                                                                 dense_74[17][0]                  \n",
            "                                                                 dense_74[18][0]                  \n",
            "                                                                 dense_74[19][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_198 (Dropout)           (None, 20, 300)      0           embedding_23[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "reshape_7 (Reshape)             (None, 20, 200)      0           concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_35 (Bidirectional (None, 20, 256)      440320      dropout_198[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_220 (Dropout)           (None, 20, 200)      0           reshape_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_199 (Dropout)           (None, 20, 256)      0           bidirectional_35[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "cu_dnnlstm_8 (CuDNNLSTM)        (None, 20, 200)      321600      dropout_220[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_36 (Bidirectional (None, 20, 200)      286400      dropout_199[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_37 (Bidirectional (None, 20, 400)      643200      cu_dnnlstm_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 20, 600)      0           bidirectional_36[0][0]           \n",
            "                                                                 bidirectional_37[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_221 (Dropout)           (None, 20, 600)      0           concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_38 (Bidirectional (None, 20, 512)      1757184     dropout_221[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_222 (Dropout)           (None, 20, 512)      0           bidirectional_38[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_39 (Bidirectional (None, 20, 256)      657408      dropout_222[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 20, 256)      0           bidirectional_39[0][0]           \n",
            "                                                                 bidirectional_35[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_7 (TimeDistrib (None, 20, 17)       4369        add_2[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 20,998,281\n",
            "Trainable params: 20,998,281\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A2bT0jIVRW_K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Version 2 Tweaked? Mehr bilstms und mehr Dropout\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D, Flatten\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, TimeDistributed, Bidirectional, CuDNNLSTM\n",
        "from keras.layers.merge import add\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences=Embedding(50491,300, weights=[weights])(sequence_input)\n",
        "x = Dropout(0.77)(embedded_sequences)\n",
        "xn = Bidirectional(CuDNNLSTM(128,return_sequences=True))(x)\n",
        "x = Dropout(0.77)(xn)\n",
        "x = Bidirectional(CuDNNLSTM(128,return_sequences=True))(x) # ich habe die beiden mal deutlich reduziert\n",
        "\n",
        "aux_inputs=[Input(shape=(CHARSEQLEN,), dtype=\"int32\") for i in range(MAX_SEQUENCE_LENGTH)]\n",
        "aux_embedding=Embedding(len(aux_tokenizer.word_index),30)\n",
        "aux_embedded=[aux_embedding(inp) for inp in aux_inputs]\n",
        "aux_lstm=Bidirectional(CuDNNLSTM(200))#,return_sequences=True)\n",
        "aux_x = [Dropout(0.4)(aux_lstm(vals)) for vals in aux_embedded]\n",
        "dense=Dense(200)\n",
        "aux_x = [dense(vals) for vals in aux_x]\n",
        "x2=keras.layers.concatenate(aux_x) #Über welche Achse machen wir das denn? #hier könnten wir auch den Output von einem LSTM mit drankleben, das auf Input 1 lernt\n",
        "#Hier könnte noch ein Dense Layer rein\n",
        "x2=Reshape((MAX_SEQUENCE_LENGTH,200))(x2)\n",
        "x2=Dropout(0.6)(x2)\n",
        "x2=Bidirectional(CuDNNLSTM(200, return_sequences=True))(x2)\n",
        "x2=Dropout(0.6)(x2)\n",
        "x2=Bidirectional(CuDNNLSTM(200,return_sequences=True))(x2)\n",
        "x = keras.layers.concatenate([x, x2])\n",
        "#x=x2\n",
        "x = Dropout(0.6)(x)\n",
        "x = Bidirectional(CuDNNLSTM(256,return_sequences=True))(x)\n",
        "x = Dropout(0.55)(x)\n",
        "x = Bidirectional(CuDNNLSTM(128,return_sequences=True))(x)\n",
        "x = add([x,xn])\n",
        "preds = TimeDistributed(Dense(numoftags+1,activation=\"softmax\"))(x)\n",
        "\n",
        "model = Model([sequence_input]+aux_inputs, preds)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
        "histy=model.fit([X1]+auxseq, Y, epochs=25, batch_size=512, validation_split=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AQ3wVOqkY7ew",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Diese Version hier können wir bei Erfolg auch zu kontextualisierten Word repräsentationen weiterentwickeln\n",
        "#Der Input erzeugt ja sozusagen Word embeddings, die die Wortart lernen und der mittlere Layer disambiguiert\n",
        "#Daher können wir das vermutlich auch an anderen Tasks trainieren\n",
        "#Also entweder großen Sequenzannotationen\n",
        "#Würden Übersetzungen auch gehen? Ich bin mir nicht ganz sicher, weil dieses Dimng hier ja darauf abziehlt von einem Wort auf ein Wort abzubilden.\n",
        "#Variable Outputlänge erscheint sehr schwer möglich in diese Art.\n",
        "\n",
        "#Natürlich kann man an den großen post-concat Vektor einen Decoder drankleben ;)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZB0AuH8FR-2T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 23615
        },
        "outputId": "f4432c16-bd1d-4db4-f8ca-f78493d0c598"
      },
      "cell_type": "code",
      "source": [
        "#Version 2 Tweaked? Mehr bilstms und mehr Dropout. Und ein bisschen komisches\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D, Flatten, Reshape\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, TimeDistributed, Bidirectional, CuDNNLSTM\n",
        "from keras.layers.merge import add\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences=Embedding(50491,300, weights=[weights])(sequence_input)\n",
        "x = Dropout(0.77)(embedded_sequences)\n",
        "xn = Bidirectional(CuDNNLSTM(128,return_sequences=True))(x)\n",
        "x = Dropout(0.77)(xn)\n",
        "x = Bidirectional(CuDNNLSTM(128,return_sequences=True))(x) # ich habe die beiden mal deutlich reduziert\n",
        "\n",
        "aux_inputs=[Input(shape=(CHARSEQLEN,), dtype=\"int32\") for i in range(MAX_SEQUENCE_LENGTH)]\n",
        "aux_embedding=Embedding(len(aux_tokenizer.word_index),30)\n",
        "aux_embedded=[aux_embedding(inp) for inp in aux_inputs]\n",
        "aux_lstm=Bidirectional(CuDNNLSTM(200,return_sequences=True))\n",
        "aux_x = [Dropout(0.3)(aux_lstm(vals)) for vals in aux_embedded]\n",
        "aux_lstme=Bidirectional(CuDNNLSTM(200))\n",
        "aux_x = [Dropout(0.3)(aux_lstme(vals)) for vals in aux_x]\n",
        "dense=Dense(100)\n",
        "aux_x = [dense(vals) for vals in aux_x]\n",
        "x2=keras.layers.concatenate(aux_x) #Über welche Achse machen wir das denn?\n",
        "#Wollen wir hier noch etwas dense sein? Und ggf noch den LSM Output von X reinkippen, also sowas wie\n",
        "#auxaux = CuDNNLSTM(100)(x)\n",
        "#x2=Dense(MAX_SEQUENCE_LENGTH*100)(x2)\n",
        "#x2=keras.layers.concatenate([x2,auxaux])\n",
        "#x2=Dense(MAX_SEQUENCE_LENGTH*100)(x2)\n",
        "x2=Reshape((MAX_SEQUENCE_LENGTH,100))(x2)\n",
        "x2=Dropout(0.5)(x2)\n",
        "x2=Bidirectional(CuDNNLSTM(200, return_sequences=True))(x2)\n",
        "x2=Dropout(0.6)(x2)\n",
        "x2=Bidirectional(CuDNNLSTM(200,return_sequences=True))(x2)\n",
        "#x = keras.layers.concatenate([x, x2])\n",
        "x=x2 #Nun trainieren wir das Modell allein auf den Repräsentationen \n",
        "x = Dropout(0.6)(x)\n",
        "x = Bidirectional(CuDNNLSTM(256,return_sequences=True))(x)\n",
        "x = Dropout(0.55)(x)\n",
        "x = Bidirectional(CuDNNLSTM(128,return_sequences=True))(x)\n",
        "#x = add([x,xn])\n",
        "preds = TimeDistributed(Dense(numoftags+1,activation=\"softmax\"))(x)\n",
        "\n",
        "model = Model([sequence_input]+aux_inputs, preds)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
        "histi=model.fit([X1]+auxseq, Y, epochs=1000, batch_size=512, validation_split=0.1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 14030 samples, validate on 1559 samples\n",
            "Epoch 1/1000\n",
            "14030/14030 [==============================] - 81s 6ms/step - loss: 2.1686 - acc: 0.5081 - val_loss: 1.6914 - val_acc: 0.6115\n",
            "Epoch 2/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.7375 - acc: 0.5845 - val_loss: 1.7041 - val_acc: 0.6276\n",
            "Epoch 3/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.6887 - acc: 0.5888 - val_loss: 1.5836 - val_acc: 0.6240\n",
            "Epoch 4/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.6833 - acc: 0.5895 - val_loss: 1.5323 - val_acc: 0.6276\n",
            "Epoch 5/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.8304 - acc: 0.5709 - val_loss: 1.5523 - val_acc: 0.6300\n",
            "Epoch 6/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 1.6374 - acc: 0.5943 - val_loss: 1.5839 - val_acc: 0.6213\n",
            "Epoch 7/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.7035 - acc: 0.5802 - val_loss: 1.5241 - val_acc: 0.6313\n",
            "Epoch 8/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.6471 - acc: 0.5875 - val_loss: 1.5318 - val_acc: 0.6273\n",
            "Epoch 9/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.6240 - acc: 0.5934 - val_loss: 1.3487 - val_acc: 0.6447\n",
            "Epoch 10/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.4973 - acc: 0.6061 - val_loss: 1.2900 - val_acc: 0.6553\n",
            "Epoch 11/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.4231 - acc: 0.6166 - val_loss: 1.4703 - val_acc: 0.5986\n",
            "Epoch 12/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.4090 - acc: 0.6211 - val_loss: 1.2193 - val_acc: 0.6723\n",
            "Epoch 13/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 1.3508 - acc: 0.6306 - val_loss: 1.2054 - val_acc: 0.6651\n",
            "Epoch 14/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.3321 - acc: 0.6319 - val_loss: 1.2031 - val_acc: 0.6687\n",
            "Epoch 15/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.3200 - acc: 0.6333 - val_loss: 1.4780 - val_acc: 0.5979\n",
            "Epoch 16/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.3060 - acc: 0.6356 - val_loss: 1.2357 - val_acc: 0.6626\n",
            "Epoch 17/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2965 - acc: 0.6376 - val_loss: 1.2722 - val_acc: 0.6523\n",
            "Epoch 18/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2885 - acc: 0.6386 - val_loss: 1.2312 - val_acc: 0.6620\n",
            "Epoch 19/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2953 - acc: 0.6375 - val_loss: 1.1564 - val_acc: 0.6777\n",
            "Epoch 20/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2667 - acc: 0.6428 - val_loss: 1.1919 - val_acc: 0.6736\n",
            "Epoch 21/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2759 - acc: 0.6410 - val_loss: 1.1623 - val_acc: 0.6796\n",
            "Epoch 22/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 1.2642 - acc: 0.6431 - val_loss: 1.1690 - val_acc: 0.6782\n",
            "Epoch 23/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2612 - acc: 0.6436 - val_loss: 1.1587 - val_acc: 0.6738\n",
            "Epoch 24/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2479 - acc: 0.6450 - val_loss: 1.1460 - val_acc: 0.6780\n",
            "Epoch 25/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2502 - acc: 0.6449 - val_loss: 1.1592 - val_acc: 0.6740\n",
            "Epoch 26/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2556 - acc: 0.6441 - val_loss: 1.1509 - val_acc: 0.6740\n",
            "Epoch 27/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2493 - acc: 0.6447 - val_loss: 1.1418 - val_acc: 0.6775\n",
            "Epoch 28/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2430 - acc: 0.6460 - val_loss: 1.1633 - val_acc: 0.6732\n",
            "Epoch 29/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2375 - acc: 0.6467 - val_loss: 1.1418 - val_acc: 0.6774\n",
            "Epoch 30/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2576 - acc: 0.6467 - val_loss: 1.1548 - val_acc: 0.6783\n",
            "Epoch 31/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2345 - acc: 0.6471 - val_loss: 1.1515 - val_acc: 0.6788\n",
            "Epoch 32/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2349 - acc: 0.6472 - val_loss: 1.1439 - val_acc: 0.6749\n",
            "Epoch 33/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2328 - acc: 0.6477 - val_loss: 1.1318 - val_acc: 0.6787\n",
            "Epoch 34/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2310 - acc: 0.6474 - val_loss: 1.1351 - val_acc: 0.6783\n",
            "Epoch 35/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2260 - acc: 0.6487 - val_loss: 1.1354 - val_acc: 0.6772\n",
            "Epoch 36/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2292 - acc: 0.6478 - val_loss: 1.1228 - val_acc: 0.6804\n",
            "Epoch 37/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2296 - acc: 0.6477 - val_loss: 1.1634 - val_acc: 0.6717\n",
            "Epoch 38/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2220 - acc: 0.6491 - val_loss: 1.1351 - val_acc: 0.6822\n",
            "Epoch 39/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2281 - acc: 0.6484 - val_loss: 1.1294 - val_acc: 0.6785\n",
            "Epoch 40/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 1.2217 - acc: 0.6489 - val_loss: 1.1580 - val_acc: 0.6756\n",
            "Epoch 41/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2175 - acc: 0.6501 - val_loss: 1.1525 - val_acc: 0.6767\n",
            "Epoch 42/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2175 - acc: 0.6499 - val_loss: 1.1428 - val_acc: 0.6770\n",
            "Epoch 43/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 1.2148 - acc: 0.6503 - val_loss: 1.1619 - val_acc: 0.6759\n",
            "Epoch 44/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2167 - acc: 0.6498 - val_loss: 1.1169 - val_acc: 0.6816\n",
            "Epoch 45/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2146 - acc: 0.6504 - val_loss: 1.1423 - val_acc: 0.6804\n",
            "Epoch 46/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2150 - acc: 0.6502 - val_loss: 1.1162 - val_acc: 0.6851\n",
            "Epoch 47/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2117 - acc: 0.6509 - val_loss: 1.1468 - val_acc: 0.6794\n",
            "Epoch 48/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2143 - acc: 0.6504 - val_loss: 1.1145 - val_acc: 0.6832\n",
            "Epoch 49/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2089 - acc: 0.6515 - val_loss: 1.1199 - val_acc: 0.6806\n",
            "Epoch 50/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 1.2106 - acc: 0.6508 - val_loss: 1.1357 - val_acc: 0.6780\n",
            "Epoch 51/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2059 - acc: 0.6521 - val_loss: 1.1789 - val_acc: 0.6621\n",
            "Epoch 52/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2089 - acc: 0.6514 - val_loss: 1.1810 - val_acc: 0.6661\n",
            "Epoch 53/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2070 - acc: 0.6518 - val_loss: 1.1761 - val_acc: 0.6670\n",
            "Epoch 54/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2062 - acc: 0.6520 - val_loss: 1.1385 - val_acc: 0.6761\n",
            "Epoch 55/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2043 - acc: 0.6522 - val_loss: 1.1439 - val_acc: 0.6784\n",
            "Epoch 56/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2041 - acc: 0.6522 - val_loss: 1.1265 - val_acc: 0.6815\n",
            "Epoch 57/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2143 - acc: 0.6513 - val_loss: 1.1118 - val_acc: 0.6832\n",
            "Epoch 58/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2011 - acc: 0.6529 - val_loss: 1.1157 - val_acc: 0.6835\n",
            "Epoch 59/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2036 - acc: 0.6524 - val_loss: 1.1607 - val_acc: 0.6778\n",
            "Epoch 60/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2057 - acc: 0.6522 - val_loss: 1.1111 - val_acc: 0.6859\n",
            "Epoch 61/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.1990 - acc: 0.6532 - val_loss: 1.1238 - val_acc: 0.6817\n",
            "Epoch 62/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.1986 - acc: 0.6531 - val_loss: 1.1102 - val_acc: 0.6845\n",
            "Epoch 63/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.1968 - acc: 0.6538 - val_loss: 1.1114 - val_acc: 0.6842\n",
            "Epoch 64/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2050 - acc: 0.6524 - val_loss: 1.1097 - val_acc: 0.6860\n",
            "Epoch 65/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.1941 - acc: 0.6541 - val_loss: 1.1052 - val_acc: 0.6857\n",
            "Epoch 66/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.1921 - acc: 0.6546 - val_loss: 1.1277 - val_acc: 0.6800\n",
            "Epoch 67/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.1945 - acc: 0.6539 - val_loss: 1.1153 - val_acc: 0.6836\n",
            "Epoch 68/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.2061 - acc: 0.6520 - val_loss: 1.1089 - val_acc: 0.6852\n",
            "Epoch 69/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.1855 - acc: 0.6553 - val_loss: 1.0993 - val_acc: 0.6858\n",
            "Epoch 70/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.1792 - acc: 0.6560 - val_loss: 1.1186 - val_acc: 0.6846\n",
            "Epoch 71/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.1685 - acc: 0.6575 - val_loss: 1.0773 - val_acc: 0.6886\n",
            "Epoch 72/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.1696 - acc: 0.6566 - val_loss: 1.1140 - val_acc: 0.6827\n",
            "Epoch 73/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.1590 - acc: 0.6587 - val_loss: 1.0727 - val_acc: 0.6876\n",
            "Epoch 74/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.1485 - acc: 0.6608 - val_loss: 1.0645 - val_acc: 0.6909\n",
            "Epoch 75/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.1428 - acc: 0.6621 - val_loss: 1.0522 - val_acc: 0.6950\n",
            "Epoch 76/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.1337 - acc: 0.6640 - val_loss: 1.0446 - val_acc: 0.6958\n",
            "Epoch 77/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.1417 - acc: 0.6628 - val_loss: 1.0605 - val_acc: 0.6918\n",
            "Epoch 78/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.1203 - acc: 0.6668 - val_loss: 1.0324 - val_acc: 0.6987\n",
            "Epoch 79/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.1120 - acc: 0.6689 - val_loss: 1.0326 - val_acc: 0.6996\n",
            "Epoch 80/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.1050 - acc: 0.6703 - val_loss: 1.0208 - val_acc: 0.7003\n",
            "Epoch 81/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.1038 - acc: 0.6711 - val_loss: 1.0222 - val_acc: 0.6990\n",
            "Epoch 82/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.1057 - acc: 0.6716 - val_loss: 1.0028 - val_acc: 0.7038\n",
            "Epoch 83/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.0819 - acc: 0.6754 - val_loss: 1.0008 - val_acc: 0.7050\n",
            "Epoch 84/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.0987 - acc: 0.6725 - val_loss: 1.0388 - val_acc: 0.6964\n",
            "Epoch 85/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.0659 - acc: 0.6782 - val_loss: 0.9900 - val_acc: 0.7053\n",
            "Epoch 86/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.0733 - acc: 0.6764 - val_loss: 0.9878 - val_acc: 0.7076\n",
            "Epoch 87/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.0670 - acc: 0.6779 - val_loss: 0.9779 - val_acc: 0.7078\n",
            "Epoch 88/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.0558 - acc: 0.6797 - val_loss: 0.9907 - val_acc: 0.7013\n",
            "Epoch 89/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.0485 - acc: 0.6810 - val_loss: 1.0003 - val_acc: 0.7026\n",
            "Epoch 90/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.0503 - acc: 0.6808 - val_loss: 0.9573 - val_acc: 0.7131\n",
            "Epoch 91/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.0291 - acc: 0.6856 - val_loss: 0.9489 - val_acc: 0.7141\n",
            "Epoch 92/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.0255 - acc: 0.6864 - val_loss: 0.9392 - val_acc: 0.7169\n",
            "Epoch 93/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.0325 - acc: 0.6859 - val_loss: 0.9580 - val_acc: 0.7120\n",
            "Epoch 94/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.0217 - acc: 0.6887 - val_loss: 0.9304 - val_acc: 0.7211\n",
            "Epoch 95/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.0024 - acc: 0.6924 - val_loss: 0.9375 - val_acc: 0.7178\n",
            "Epoch 96/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.9897 - acc: 0.6959 - val_loss: 0.9307 - val_acc: 0.7220\n",
            "Epoch 97/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 1.0183 - acc: 0.6927 - val_loss: 0.9269 - val_acc: 0.7222\n",
            "Epoch 98/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.9748 - acc: 0.7011 - val_loss: 0.9033 - val_acc: 0.7288\n",
            "Epoch 99/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.9680 - acc: 0.7023 - val_loss: 0.9071 - val_acc: 0.7263\n",
            "Epoch 100/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.9631 - acc: 0.7045 - val_loss: 0.9143 - val_acc: 0.7237\n",
            "Epoch 101/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.9572 - acc: 0.7060 - val_loss: 0.8779 - val_acc: 0.7348\n",
            "Epoch 102/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.9446 - acc: 0.7102 - val_loss: 0.8632 - val_acc: 0.7407\n",
            "Epoch 103/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.9367 - acc: 0.7119 - val_loss: 0.8516 - val_acc: 0.7441\n",
            "Epoch 104/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.9274 - acc: 0.7152 - val_loss: 0.8635 - val_acc: 0.7398\n",
            "Epoch 105/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.9227 - acc: 0.7167 - val_loss: 0.8335 - val_acc: 0.7499\n",
            "Epoch 106/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.9306 - acc: 0.7161 - val_loss: 0.8271 - val_acc: 0.7509\n",
            "Epoch 107/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.8943 - acc: 0.7250 - val_loss: 0.8223 - val_acc: 0.7535\n",
            "Epoch 108/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.8894 - acc: 0.7262 - val_loss: 0.8132 - val_acc: 0.7554\n",
            "Epoch 109/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.8860 - acc: 0.7272 - val_loss: 0.8057 - val_acc: 0.7589\n",
            "Epoch 110/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.8754 - acc: 0.7304 - val_loss: 0.7954 - val_acc: 0.7594\n",
            "Epoch 111/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.8661 - acc: 0.7338 - val_loss: 0.7883 - val_acc: 0.7626\n",
            "Epoch 112/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.8600 - acc: 0.7352 - val_loss: 0.7952 - val_acc: 0.7607\n",
            "Epoch 113/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.8518 - acc: 0.7368 - val_loss: 0.7764 - val_acc: 0.7652\n",
            "Epoch 114/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.8530 - acc: 0.7370 - val_loss: 0.7652 - val_acc: 0.7700\n",
            "Epoch 115/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.8343 - acc: 0.7427 - val_loss: 0.7597 - val_acc: 0.7713\n",
            "Epoch 116/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.8489 - acc: 0.7400 - val_loss: 0.7526 - val_acc: 0.7730\n",
            "Epoch 117/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.8173 - acc: 0.7474 - val_loss: 0.7467 - val_acc: 0.7750\n",
            "Epoch 118/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.8135 - acc: 0.7488 - val_loss: 0.7458 - val_acc: 0.7749\n",
            "Epoch 119/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.8053 - acc: 0.7508 - val_loss: 0.7529 - val_acc: 0.7728\n",
            "Epoch 120/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.8008 - acc: 0.7526 - val_loss: 0.7297 - val_acc: 0.7798\n",
            "Epoch 121/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.7906 - acc: 0.7555 - val_loss: 0.7376 - val_acc: 0.7786\n",
            "Epoch 122/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.7889 - acc: 0.7562 - val_loss: 0.7063 - val_acc: 0.7870\n",
            "Epoch 123/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.7746 - acc: 0.7596 - val_loss: 0.7308 - val_acc: 0.7798\n",
            "Epoch 124/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.8283 - acc: 0.7495 - val_loss: 0.7005 - val_acc: 0.7894\n",
            "Epoch 125/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.7511 - acc: 0.7669 - val_loss: 0.7488 - val_acc: 0.7732\n",
            "Epoch 126/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.7588 - acc: 0.7642 - val_loss: 0.6944 - val_acc: 0.7905\n",
            "Epoch 127/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.7511 - acc: 0.7660 - val_loss: 0.6971 - val_acc: 0.7898\n",
            "Epoch 128/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.7432 - acc: 0.7693 - val_loss: 0.6843 - val_acc: 0.7938\n",
            "Epoch 129/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.7400 - acc: 0.7695 - val_loss: 0.6725 - val_acc: 0.7964\n",
            "Epoch 130/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.7314 - acc: 0.7725 - val_loss: 0.6691 - val_acc: 0.7972\n",
            "Epoch 131/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.7174 - acc: 0.7762 - val_loss: 0.6748 - val_acc: 0.7953\n",
            "Epoch 132/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.7196 - acc: 0.7758 - val_loss: 0.6631 - val_acc: 0.7998\n",
            "Epoch 133/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.7130 - acc: 0.7780 - val_loss: 0.6634 - val_acc: 0.8017\n",
            "Epoch 134/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.7043 - acc: 0.7804 - val_loss: 0.6553 - val_acc: 0.8029\n",
            "Epoch 135/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.6975 - acc: 0.7824 - val_loss: 0.6334 - val_acc: 0.8080\n",
            "Epoch 136/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.6875 - acc: 0.7852 - val_loss: 0.6306 - val_acc: 0.8096\n",
            "Epoch 137/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.6839 - acc: 0.7864 - val_loss: 0.6338 - val_acc: 0.8094\n",
            "Epoch 138/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.6817 - acc: 0.7877 - val_loss: 0.6138 - val_acc: 0.8152\n",
            "Epoch 139/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.6639 - acc: 0.7929 - val_loss: 0.6187 - val_acc: 0.8133\n",
            "Epoch 140/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.6642 - acc: 0.7928 - val_loss: 0.6100 - val_acc: 0.8153\n",
            "Epoch 141/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.6557 - acc: 0.7954 - val_loss: 0.6087 - val_acc: 0.8158\n",
            "Epoch 142/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.6510 - acc: 0.7974 - val_loss: 0.5959 - val_acc: 0.8192\n",
            "Epoch 143/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.6427 - acc: 0.7991 - val_loss: 0.6118 - val_acc: 0.8142\n",
            "Epoch 144/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.6395 - acc: 0.8006 - val_loss: 0.5823 - val_acc: 0.8240\n",
            "Epoch 145/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.6441 - acc: 0.8000 - val_loss: 0.5796 - val_acc: 0.8259\n",
            "Epoch 146/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.6202 - acc: 0.8062 - val_loss: 0.5857 - val_acc: 0.8228\n",
            "Epoch 147/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.6196 - acc: 0.8071 - val_loss: 0.5677 - val_acc: 0.8291\n",
            "Epoch 148/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.6111 - acc: 0.8094 - val_loss: 0.5836 - val_acc: 0.8245\n",
            "Epoch 149/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.6054 - acc: 0.8114 - val_loss: 0.5571 - val_acc: 0.8317\n",
            "Epoch 150/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.6005 - acc: 0.8132 - val_loss: 0.5453 - val_acc: 0.8348\n",
            "Epoch 151/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.5931 - acc: 0.8150 - val_loss: 0.5583 - val_acc: 0.8320\n",
            "Epoch 152/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.5844 - acc: 0.8178 - val_loss: 0.5478 - val_acc: 0.8344\n",
            "Epoch 153/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.5823 - acc: 0.8189 - val_loss: 0.5285 - val_acc: 0.8403\n",
            "Epoch 154/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.5744 - acc: 0.8214 - val_loss: 0.5310 - val_acc: 0.8410\n",
            "Epoch 155/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.5702 - acc: 0.8228 - val_loss: 0.5135 - val_acc: 0.8449\n",
            "Epoch 156/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.5706 - acc: 0.8229 - val_loss: 0.5126 - val_acc: 0.8470\n",
            "Epoch 157/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.5524 - acc: 0.8286 - val_loss: 0.5161 - val_acc: 0.8447\n",
            "Epoch 158/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.5482 - acc: 0.8303 - val_loss: 0.4961 - val_acc: 0.8509\n",
            "Epoch 159/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.5472 - acc: 0.8306 - val_loss: 0.4931 - val_acc: 0.8536\n",
            "Epoch 160/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.5355 - acc: 0.8343 - val_loss: 0.4933 - val_acc: 0.8523\n",
            "Epoch 161/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.5304 - acc: 0.8356 - val_loss: 0.4968 - val_acc: 0.8498\n",
            "Epoch 162/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.5262 - acc: 0.8367 - val_loss: 0.4820 - val_acc: 0.8572\n",
            "Epoch 163/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.5205 - acc: 0.8394 - val_loss: 0.4892 - val_acc: 0.8555\n",
            "Epoch 164/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.5165 - acc: 0.8403 - val_loss: 0.4743 - val_acc: 0.8591\n",
            "Epoch 165/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.5044 - acc: 0.8443 - val_loss: 0.4604 - val_acc: 0.8645\n",
            "Epoch 166/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.5073 - acc: 0.8435 - val_loss: 0.4594 - val_acc: 0.8642\n",
            "Epoch 167/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.5022 - acc: 0.8457 - val_loss: 0.4864 - val_acc: 0.8567\n",
            "Epoch 168/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.4943 - acc: 0.8494 - val_loss: 0.4433 - val_acc: 0.8697\n",
            "Epoch 169/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.4877 - acc: 0.8496 - val_loss: 0.4404 - val_acc: 0.8702\n",
            "Epoch 170/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.4770 - acc: 0.8536 - val_loss: 0.4300 - val_acc: 0.8744\n",
            "Epoch 171/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.4798 - acc: 0.8530 - val_loss: 0.4307 - val_acc: 0.8741\n",
            "Epoch 172/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.4689 - acc: 0.8560 - val_loss: 0.4356 - val_acc: 0.8725\n",
            "Epoch 173/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.4688 - acc: 0.8557 - val_loss: 0.4209 - val_acc: 0.8761\n",
            "Epoch 174/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.4595 - acc: 0.8585 - val_loss: 0.4281 - val_acc: 0.8749\n",
            "Epoch 175/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.4602 - acc: 0.8588 - val_loss: 0.4152 - val_acc: 0.8777\n",
            "Epoch 176/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.4451 - acc: 0.8629 - val_loss: 0.4063 - val_acc: 0.8801\n",
            "Epoch 177/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.4479 - acc: 0.8622 - val_loss: 0.4171 - val_acc: 0.8777\n",
            "Epoch 178/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.4373 - acc: 0.8655 - val_loss: 0.4020 - val_acc: 0.8826\n",
            "Epoch 179/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.4486 - acc: 0.8645 - val_loss: 0.3947 - val_acc: 0.8844\n",
            "Epoch 180/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.4333 - acc: 0.8668 - val_loss: 0.3937 - val_acc: 0.8846\n",
            "Epoch 181/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.4204 - acc: 0.8708 - val_loss: 0.3881 - val_acc: 0.8867\n",
            "Epoch 182/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.4225 - acc: 0.8697 - val_loss: 0.3916 - val_acc: 0.8860\n",
            "Epoch 183/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.4193 - acc: 0.8715 - val_loss: 0.3791 - val_acc: 0.8882\n",
            "Epoch 184/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.4071 - acc: 0.8747 - val_loss: 0.3868 - val_acc: 0.8859\n",
            "Epoch 185/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.4069 - acc: 0.8750 - val_loss: 0.3711 - val_acc: 0.8927\n",
            "Epoch 186/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3996 - acc: 0.8768 - val_loss: 0.3736 - val_acc: 0.8910\n",
            "Epoch 187/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3982 - acc: 0.8776 - val_loss: 0.3626 - val_acc: 0.8945\n",
            "Epoch 188/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3908 - acc: 0.8796 - val_loss: 0.3701 - val_acc: 0.8923\n",
            "Epoch 189/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3886 - acc: 0.8805 - val_loss: 0.3698 - val_acc: 0.8906\n",
            "Epoch 190/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.3903 - acc: 0.8803 - val_loss: 0.3548 - val_acc: 0.8962\n",
            "Epoch 191/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3747 - acc: 0.8843 - val_loss: 0.3480 - val_acc: 0.8980\n",
            "Epoch 192/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3772 - acc: 0.8840 - val_loss: 0.3446 - val_acc: 0.8998\n",
            "Epoch 193/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3728 - acc: 0.8852 - val_loss: 0.3485 - val_acc: 0.8985\n",
            "Epoch 194/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3641 - acc: 0.8877 - val_loss: 0.3431 - val_acc: 0.8985\n",
            "Epoch 195/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3662 - acc: 0.8871 - val_loss: 0.3397 - val_acc: 0.8991\n",
            "Epoch 196/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3611 - acc: 0.8885 - val_loss: 0.3407 - val_acc: 0.8997\n",
            "Epoch 197/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3628 - acc: 0.8889 - val_loss: 0.3299 - val_acc: 0.9035\n",
            "Epoch 198/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3547 - acc: 0.8906 - val_loss: 0.3330 - val_acc: 0.9024\n",
            "Epoch 199/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.4263 - acc: 0.8814 - val_loss: 0.3249 - val_acc: 0.9042\n",
            "Epoch 200/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3465 - acc: 0.8934 - val_loss: 0.3236 - val_acc: 0.9056\n",
            "Epoch 201/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3434 - acc: 0.8938 - val_loss: 0.3307 - val_acc: 0.9031\n",
            "Epoch 202/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3357 - acc: 0.8966 - val_loss: 0.3272 - val_acc: 0.9045\n",
            "Epoch 203/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3338 - acc: 0.8962 - val_loss: 0.3238 - val_acc: 0.9056\n",
            "Epoch 204/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3320 - acc: 0.8971 - val_loss: 0.3138 - val_acc: 0.9083\n",
            "Epoch 205/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3337 - acc: 0.8970 - val_loss: 0.3252 - val_acc: 0.9048\n",
            "Epoch 206/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3210 - acc: 0.9006 - val_loss: 0.3415 - val_acc: 0.8989\n",
            "Epoch 207/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3254 - acc: 0.8992 - val_loss: 0.3098 - val_acc: 0.9097\n",
            "Epoch 208/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3132 - acc: 0.9026 - val_loss: 0.3038 - val_acc: 0.9116\n",
            "Epoch 209/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3155 - acc: 0.9025 - val_loss: 0.3333 - val_acc: 0.9089\n",
            "Epoch 210/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3101 - acc: 0.9042 - val_loss: 0.2970 - val_acc: 0.9130\n",
            "Epoch 211/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3064 - acc: 0.9046 - val_loss: 0.3005 - val_acc: 0.9124\n",
            "Epoch 212/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.3104 - acc: 0.9039 - val_loss: 0.2967 - val_acc: 0.9145\n",
            "Epoch 213/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2981 - acc: 0.9069 - val_loss: 0.3006 - val_acc: 0.9113\n",
            "Epoch 214/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2996 - acc: 0.9070 - val_loss: 0.2954 - val_acc: 0.9135\n",
            "Epoch 215/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2920 - acc: 0.9089 - val_loss: 0.2865 - val_acc: 0.9171\n",
            "Epoch 216/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2990 - acc: 0.9071 - val_loss: 0.2849 - val_acc: 0.9169\n",
            "Epoch 217/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2942 - acc: 0.9090 - val_loss: 0.2969 - val_acc: 0.9130\n",
            "Epoch 218/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2825 - acc: 0.9122 - val_loss: 0.2838 - val_acc: 0.9172\n",
            "Epoch 219/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2854 - acc: 0.9110 - val_loss: 0.2814 - val_acc: 0.9177\n",
            "Epoch 220/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2829 - acc: 0.9118 - val_loss: 0.2763 - val_acc: 0.9200\n",
            "Epoch 221/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2823 - acc: 0.9121 - val_loss: 0.2775 - val_acc: 0.9189\n",
            "Epoch 222/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2784 - acc: 0.9133 - val_loss: 0.2790 - val_acc: 0.9188\n",
            "Epoch 223/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2760 - acc: 0.9138 - val_loss: 0.2741 - val_acc: 0.9208\n",
            "Epoch 224/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2931 - acc: 0.9122 - val_loss: 0.2679 - val_acc: 0.9232\n",
            "Epoch 225/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2640 - acc: 0.9175 - val_loss: 0.2730 - val_acc: 0.9209\n",
            "Epoch 226/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2658 - acc: 0.9170 - val_loss: 0.2710 - val_acc: 0.9220\n",
            "Epoch 227/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2637 - acc: 0.9180 - val_loss: 0.2701 - val_acc: 0.9222\n",
            "Epoch 228/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2610 - acc: 0.9182 - val_loss: 0.2687 - val_acc: 0.9224\n",
            "Epoch 229/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2617 - acc: 0.9182 - val_loss: 0.2616 - val_acc: 0.9241\n",
            "Epoch 230/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.2561 - acc: 0.9202 - val_loss: 0.2622 - val_acc: 0.9246\n",
            "Epoch 231/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.2540 - acc: 0.9204 - val_loss: 0.2595 - val_acc: 0.9249\n",
            "Epoch 232/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.2611 - acc: 0.9192 - val_loss: 0.2596 - val_acc: 0.9247\n",
            "Epoch 233/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2460 - acc: 0.9228 - val_loss: 0.2570 - val_acc: 0.9259\n",
            "Epoch 234/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2471 - acc: 0.9223 - val_loss: 0.2665 - val_acc: 0.9232\n",
            "Epoch 235/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.2488 - acc: 0.9221 - val_loss: 0.2571 - val_acc: 0.9256\n",
            "Epoch 236/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.2422 - acc: 0.9236 - val_loss: 0.2631 - val_acc: 0.9239\n",
            "Epoch 237/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.2489 - acc: 0.9231 - val_loss: 0.2602 - val_acc: 0.9254\n",
            "Epoch 238/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2468 - acc: 0.9232 - val_loss: 0.2549 - val_acc: 0.9262\n",
            "Epoch 239/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.2364 - acc: 0.9252 - val_loss: 0.2483 - val_acc: 0.9278\n",
            "Epoch 240/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2341 - acc: 0.9265 - val_loss: 0.2469 - val_acc: 0.9284\n",
            "Epoch 241/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2300 - acc: 0.9278 - val_loss: 0.2466 - val_acc: 0.9294\n",
            "Epoch 242/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.2331 - acc: 0.9273 - val_loss: 0.2469 - val_acc: 0.9297\n",
            "Epoch 243/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.2412 - acc: 0.9252 - val_loss: 0.2408 - val_acc: 0.9305\n",
            "Epoch 244/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.2260 - acc: 0.9294 - val_loss: 0.2429 - val_acc: 0.9301\n",
            "Epoch 245/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.2244 - acc: 0.9296 - val_loss: 0.2390 - val_acc: 0.9308\n",
            "Epoch 246/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.2250 - acc: 0.9294 - val_loss: 0.2387 - val_acc: 0.9307\n",
            "Epoch 247/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.2287 - acc: 0.9280 - val_loss: 0.2453 - val_acc: 0.9294\n",
            "Epoch 248/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.2161 - acc: 0.9318 - val_loss: 0.2360 - val_acc: 0.9322\n",
            "Epoch 249/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.2239 - acc: 0.9296 - val_loss: 0.2346 - val_acc: 0.9325\n",
            "Epoch 250/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2138 - acc: 0.9321 - val_loss: 0.2339 - val_acc: 0.9328\n",
            "Epoch 251/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2145 - acc: 0.9321 - val_loss: 0.2410 - val_acc: 0.9317\n",
            "Epoch 252/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.2220 - acc: 0.9308 - val_loss: 0.2321 - val_acc: 0.9338\n",
            "Epoch 253/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2109 - acc: 0.9329 - val_loss: 0.2311 - val_acc: 0.9336\n",
            "Epoch 254/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2089 - acc: 0.9340 - val_loss: 0.2304 - val_acc: 0.9342\n",
            "Epoch 255/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.2062 - acc: 0.9345 - val_loss: 0.2271 - val_acc: 0.9341\n",
            "Epoch 256/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.2106 - acc: 0.9338 - val_loss: 0.2240 - val_acc: 0.9356\n",
            "Epoch 257/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.2028 - acc: 0.9357 - val_loss: 0.2266 - val_acc: 0.9353\n",
            "Epoch 258/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.2019 - acc: 0.9355 - val_loss: 0.2240 - val_acc: 0.9355\n",
            "Epoch 259/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1996 - acc: 0.9367 - val_loss: 0.2238 - val_acc: 0.9364\n",
            "Epoch 260/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.2028 - acc: 0.9359 - val_loss: 0.2231 - val_acc: 0.9360\n",
            "Epoch 261/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1945 - acc: 0.9379 - val_loss: 0.2209 - val_acc: 0.9358\n",
            "Epoch 262/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.2046 - acc: 0.9361 - val_loss: 0.2236 - val_acc: 0.9366\n",
            "Epoch 263/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1893 - acc: 0.9398 - val_loss: 0.2234 - val_acc: 0.9370\n",
            "Epoch 264/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1925 - acc: 0.9389 - val_loss: 0.2223 - val_acc: 0.9370\n",
            "Epoch 265/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1923 - acc: 0.9391 - val_loss: 0.2171 - val_acc: 0.9383\n",
            "Epoch 266/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1872 - acc: 0.9403 - val_loss: 0.2177 - val_acc: 0.9384\n",
            "Epoch 267/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1920 - acc: 0.9394 - val_loss: 0.2155 - val_acc: 0.9390\n",
            "Epoch 268/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1843 - acc: 0.9413 - val_loss: 0.2167 - val_acc: 0.9383\n",
            "Epoch 269/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1883 - acc: 0.9403 - val_loss: 0.2266 - val_acc: 0.9372\n",
            "Epoch 270/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1779 - acc: 0.9433 - val_loss: 0.2186 - val_acc: 0.9386\n",
            "Epoch 271/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1846 - acc: 0.9416 - val_loss: 0.2161 - val_acc: 0.9397\n",
            "Epoch 272/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1787 - acc: 0.9433 - val_loss: 0.2126 - val_acc: 0.9403\n",
            "Epoch 273/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1784 - acc: 0.9428 - val_loss: 0.2112 - val_acc: 0.9413\n",
            "Epoch 274/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1829 - acc: 0.9424 - val_loss: 0.2121 - val_acc: 0.9405\n",
            "Epoch 275/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1732 - acc: 0.9447 - val_loss: 0.2099 - val_acc: 0.9407\n",
            "Epoch 276/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1757 - acc: 0.9438 - val_loss: 0.2133 - val_acc: 0.9407\n",
            "Epoch 277/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1722 - acc: 0.9450 - val_loss: 0.2116 - val_acc: 0.9410\n",
            "Epoch 278/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1705 - acc: 0.9452 - val_loss: 0.2130 - val_acc: 0.9406\n",
            "Epoch 279/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1732 - acc: 0.9448 - val_loss: 0.2149 - val_acc: 0.9394\n",
            "Epoch 280/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1666 - acc: 0.9467 - val_loss: 0.2076 - val_acc: 0.9425\n",
            "Epoch 281/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1676 - acc: 0.9464 - val_loss: 0.2060 - val_acc: 0.9420\n",
            "Epoch 282/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1637 - acc: 0.9475 - val_loss: 0.2048 - val_acc: 0.9429\n",
            "Epoch 283/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1651 - acc: 0.9473 - val_loss: 0.2042 - val_acc: 0.9432\n",
            "Epoch 284/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1607 - acc: 0.9485 - val_loss: 0.2057 - val_acc: 0.9432\n",
            "Epoch 285/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1648 - acc: 0.9473 - val_loss: 0.2066 - val_acc: 0.9427\n",
            "Epoch 286/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1601 - acc: 0.9487 - val_loss: 0.2049 - val_acc: 0.9435\n",
            "Epoch 287/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1594 - acc: 0.9493 - val_loss: 0.2039 - val_acc: 0.9447\n",
            "Epoch 288/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1561 - acc: 0.9501 - val_loss: 0.2079 - val_acc: 0.9428\n",
            "Epoch 289/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1568 - acc: 0.9497 - val_loss: 0.2072 - val_acc: 0.9429\n",
            "Epoch 290/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1568 - acc: 0.9495 - val_loss: 0.2140 - val_acc: 0.9430\n",
            "Epoch 291/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1527 - acc: 0.9512 - val_loss: 0.2032 - val_acc: 0.9441\n",
            "Epoch 292/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1536 - acc: 0.9509 - val_loss: 0.2087 - val_acc: 0.9440\n",
            "Epoch 293/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1650 - acc: 0.9497 - val_loss: 0.3349 - val_acc: 0.9107\n",
            "Epoch 294/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1728 - acc: 0.9478 - val_loss: 0.1965 - val_acc: 0.9466\n",
            "Epoch 295/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1474 - acc: 0.9529 - val_loss: 0.1978 - val_acc: 0.9463\n",
            "Epoch 296/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1483 - acc: 0.9524 - val_loss: 0.1991 - val_acc: 0.9469\n",
            "Epoch 297/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1482 - acc: 0.9524 - val_loss: 0.2139 - val_acc: 0.9436\n",
            "Epoch 298/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1483 - acc: 0.9529 - val_loss: 0.1956 - val_acc: 0.9474\n",
            "Epoch 299/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1452 - acc: 0.9536 - val_loss: 0.1990 - val_acc: 0.9462\n",
            "Epoch 300/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1437 - acc: 0.9538 - val_loss: 0.1958 - val_acc: 0.9473\n",
            "Epoch 301/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1422 - acc: 0.9544 - val_loss: 0.1971 - val_acc: 0.9475\n",
            "Epoch 302/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1466 - acc: 0.9531 - val_loss: 0.1961 - val_acc: 0.9472\n",
            "Epoch 303/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1433 - acc: 0.9543 - val_loss: 0.1938 - val_acc: 0.9478\n",
            "Epoch 304/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1390 - acc: 0.9554 - val_loss: 0.1937 - val_acc: 0.9471\n",
            "Epoch 305/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1397 - acc: 0.9554 - val_loss: 0.1916 - val_acc: 0.9484\n",
            "Epoch 306/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1366 - acc: 0.9560 - val_loss: 0.1990 - val_acc: 0.9475\n",
            "Epoch 307/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1395 - acc: 0.9556 - val_loss: 0.1894 - val_acc: 0.9489\n",
            "Epoch 308/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1359 - acc: 0.9566 - val_loss: 0.1937 - val_acc: 0.9477\n",
            "Epoch 309/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1348 - acc: 0.9568 - val_loss: 0.1886 - val_acc: 0.9506\n",
            "Epoch 310/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1327 - acc: 0.9573 - val_loss: 0.1937 - val_acc: 0.9488\n",
            "Epoch 311/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1318 - acc: 0.9579 - val_loss: 0.1893 - val_acc: 0.9501\n",
            "Epoch 312/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1321 - acc: 0.9578 - val_loss: 0.1868 - val_acc: 0.9498\n",
            "Epoch 313/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1338 - acc: 0.9575 - val_loss: 0.1886 - val_acc: 0.9494\n",
            "Epoch 314/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1291 - acc: 0.9586 - val_loss: 0.1893 - val_acc: 0.9499\n",
            "Epoch 315/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1292 - acc: 0.9587 - val_loss: 0.1875 - val_acc: 0.9508\n",
            "Epoch 316/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1284 - acc: 0.9591 - val_loss: 0.1884 - val_acc: 0.9498\n",
            "Epoch 317/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1261 - acc: 0.9598 - val_loss: 0.1908 - val_acc: 0.9508\n",
            "Epoch 318/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1239 - acc: 0.9598 - val_loss: 0.1869 - val_acc: 0.9509\n",
            "Epoch 319/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1233 - acc: 0.9603 - val_loss: 0.1893 - val_acc: 0.9501\n",
            "Epoch 320/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1336 - acc: 0.9588 - val_loss: 0.1924 - val_acc: 0.9501\n",
            "Epoch 321/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1207 - acc: 0.9608 - val_loss: 0.1845 - val_acc: 0.9522\n",
            "Epoch 322/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1487 - acc: 0.9572 - val_loss: 0.1832 - val_acc: 0.9517\n",
            "Epoch 323/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1172 - acc: 0.9627 - val_loss: 0.1844 - val_acc: 0.9521\n",
            "Epoch 324/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1177 - acc: 0.9623 - val_loss: 0.1827 - val_acc: 0.9523\n",
            "Epoch 325/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1207 - acc: 0.9611 - val_loss: 0.1887 - val_acc: 0.9513\n",
            "Epoch 326/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1188 - acc: 0.9622 - val_loss: 0.2426 - val_acc: 0.9424\n",
            "Epoch 327/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1213 - acc: 0.9617 - val_loss: 0.1874 - val_acc: 0.9507\n",
            "Epoch 328/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1161 - acc: 0.9624 - val_loss: 0.1922 - val_acc: 0.9503\n",
            "Epoch 329/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1164 - acc: 0.9624 - val_loss: 0.1852 - val_acc: 0.9522\n",
            "Epoch 330/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1151 - acc: 0.9630 - val_loss: 0.1819 - val_acc: 0.9532\n",
            "Epoch 331/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1141 - acc: 0.9632 - val_loss: 0.1844 - val_acc: 0.9526\n",
            "Epoch 332/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1129 - acc: 0.9637 - val_loss: 0.1821 - val_acc: 0.9532\n",
            "Epoch 333/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1116 - acc: 0.9639 - val_loss: 0.1849 - val_acc: 0.9520\n",
            "Epoch 334/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1163 - acc: 0.9631 - val_loss: 0.1847 - val_acc: 0.9527\n",
            "Epoch 335/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1103 - acc: 0.9647 - val_loss: 0.1844 - val_acc: 0.9526\n",
            "Epoch 336/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1087 - acc: 0.9650 - val_loss: 0.1831 - val_acc: 0.9536\n",
            "Epoch 337/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1150 - acc: 0.9635 - val_loss: 0.1789 - val_acc: 0.9533\n",
            "Epoch 338/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1069 - acc: 0.9653 - val_loss: 0.1871 - val_acc: 0.9528\n",
            "Epoch 339/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1073 - acc: 0.9653 - val_loss: 0.1800 - val_acc: 0.9539\n",
            "Epoch 340/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1068 - acc: 0.9655 - val_loss: 0.1854 - val_acc: 0.9536\n",
            "Epoch 341/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1080 - acc: 0.9654 - val_loss: 0.1859 - val_acc: 0.9530\n",
            "Epoch 342/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1038 - acc: 0.9667 - val_loss: 0.1875 - val_acc: 0.9518\n",
            "Epoch 343/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1051 - acc: 0.9659 - val_loss: 0.1809 - val_acc: 0.9541\n",
            "Epoch 344/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1039 - acc: 0.9665 - val_loss: 0.1770 - val_acc: 0.9547\n",
            "Epoch 345/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1026 - acc: 0.9668 - val_loss: 0.1804 - val_acc: 0.9539\n",
            "Epoch 346/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1028 - acc: 0.9667 - val_loss: 0.1808 - val_acc: 0.9539\n",
            "Epoch 347/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1027 - acc: 0.9669 - val_loss: 0.1789 - val_acc: 0.9547\n",
            "Epoch 348/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1263 - acc: 0.9638 - val_loss: 0.1770 - val_acc: 0.9549\n",
            "Epoch 349/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0975 - acc: 0.9686 - val_loss: 0.1857 - val_acc: 0.9530\n",
            "Epoch 350/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0971 - acc: 0.9685 - val_loss: 0.1825 - val_acc: 0.9547\n",
            "Epoch 351/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0987 - acc: 0.9680 - val_loss: 0.1872 - val_acc: 0.9531\n",
            "Epoch 352/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0976 - acc: 0.9684 - val_loss: 0.1796 - val_acc: 0.9557\n",
            "Epoch 353/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0974 - acc: 0.9685 - val_loss: 0.1792 - val_acc: 0.9551\n",
            "Epoch 354/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0996 - acc: 0.9682 - val_loss: 0.1787 - val_acc: 0.9550\n",
            "Epoch 355/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0954 - acc: 0.9693 - val_loss: 0.1825 - val_acc: 0.9547\n",
            "Epoch 356/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0931 - acc: 0.9699 - val_loss: 0.1814 - val_acc: 0.9551\n",
            "Epoch 357/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0953 - acc: 0.9693 - val_loss: 0.1775 - val_acc: 0.9554\n",
            "Epoch 358/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0968 - acc: 0.9685 - val_loss: 0.1763 - val_acc: 0.9563\n",
            "Epoch 359/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0922 - acc: 0.9699 - val_loss: 0.1774 - val_acc: 0.9552\n",
            "Epoch 360/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0937 - acc: 0.9699 - val_loss: 0.1786 - val_acc: 0.9554\n",
            "Epoch 361/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0928 - acc: 0.9700 - val_loss: 0.1762 - val_acc: 0.9563\n",
            "Epoch 362/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0901 - acc: 0.9708 - val_loss: 0.1823 - val_acc: 0.9550\n",
            "Epoch 363/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0908 - acc: 0.9705 - val_loss: 0.1767 - val_acc: 0.9562\n",
            "Epoch 364/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0904 - acc: 0.9705 - val_loss: 0.1740 - val_acc: 0.9566\n",
            "Epoch 365/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0886 - acc: 0.9711 - val_loss: 0.1755 - val_acc: 0.9565\n",
            "Epoch 366/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0920 - acc: 0.9706 - val_loss: 0.1762 - val_acc: 0.9570\n",
            "Epoch 367/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0945 - acc: 0.9706 - val_loss: 0.1791 - val_acc: 0.9570\n",
            "Epoch 368/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0865 - acc: 0.9718 - val_loss: 0.1820 - val_acc: 0.9557\n",
            "Epoch 369/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0863 - acc: 0.9719 - val_loss: 0.1778 - val_acc: 0.9563\n",
            "Epoch 370/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0870 - acc: 0.9721 - val_loss: 0.1788 - val_acc: 0.9567\n",
            "Epoch 371/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0882 - acc: 0.9719 - val_loss: 0.1842 - val_acc: 0.9553\n",
            "Epoch 372/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0842 - acc: 0.9727 - val_loss: 0.1850 - val_acc: 0.9554\n",
            "Epoch 373/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0851 - acc: 0.9727 - val_loss: 0.1789 - val_acc: 0.9562\n",
            "Epoch 374/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0833 - acc: 0.9728 - val_loss: 0.1795 - val_acc: 0.9565\n",
            "Epoch 375/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0937 - acc: 0.9711 - val_loss: 0.1748 - val_acc: 0.9571\n",
            "Epoch 376/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0810 - acc: 0.9738 - val_loss: 0.1779 - val_acc: 0.9570\n",
            "Epoch 377/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0836 - acc: 0.9730 - val_loss: 0.1761 - val_acc: 0.9577\n",
            "Epoch 378/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0805 - acc: 0.9737 - val_loss: 0.1774 - val_acc: 0.9570\n",
            "Epoch 379/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0812 - acc: 0.9737 - val_loss: 0.1792 - val_acc: 0.9573\n",
            "Epoch 380/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0804 - acc: 0.9738 - val_loss: 0.1770 - val_acc: 0.9577\n",
            "Epoch 381/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0813 - acc: 0.9735 - val_loss: 0.1755 - val_acc: 0.9579\n",
            "Epoch 382/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0820 - acc: 0.9737 - val_loss: 0.1763 - val_acc: 0.9577\n",
            "Epoch 383/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0792 - acc: 0.9742 - val_loss: 0.1797 - val_acc: 0.9566\n",
            "Epoch 384/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0786 - acc: 0.9746 - val_loss: 0.1795 - val_acc: 0.9574\n",
            "Epoch 385/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0789 - acc: 0.9747 - val_loss: 0.1794 - val_acc: 0.9576\n",
            "Epoch 386/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0791 - acc: 0.9745 - val_loss: 0.1756 - val_acc: 0.9590\n",
            "Epoch 387/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0769 - acc: 0.9751 - val_loss: 0.1800 - val_acc: 0.9577\n",
            "Epoch 388/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0793 - acc: 0.9746 - val_loss: 0.1792 - val_acc: 0.9575\n",
            "Epoch 389/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0756 - acc: 0.9754 - val_loss: 0.1759 - val_acc: 0.9584\n",
            "Epoch 390/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0758 - acc: 0.9753 - val_loss: 0.1773 - val_acc: 0.9583\n",
            "Epoch 391/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0749 - acc: 0.9757 - val_loss: 0.1795 - val_acc: 0.9585\n",
            "Epoch 392/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0768 - acc: 0.9753 - val_loss: 0.1761 - val_acc: 0.9582\n",
            "Epoch 393/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0765 - acc: 0.9758 - val_loss: 0.1730 - val_acc: 0.9581\n",
            "Epoch 394/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0744 - acc: 0.9758 - val_loss: 0.1734 - val_acc: 0.9591\n",
            "Epoch 395/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0732 - acc: 0.9760 - val_loss: 0.1776 - val_acc: 0.9581\n",
            "Epoch 396/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0731 - acc: 0.9763 - val_loss: 0.1768 - val_acc: 0.9581\n",
            "Epoch 397/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0724 - acc: 0.9762 - val_loss: 0.1794 - val_acc: 0.9581\n",
            "Epoch 398/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0755 - acc: 0.9760 - val_loss: 0.1716 - val_acc: 0.9592\n",
            "Epoch 399/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0703 - acc: 0.9772 - val_loss: 0.1766 - val_acc: 0.9592\n",
            "Epoch 400/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0719 - acc: 0.9764 - val_loss: 0.1805 - val_acc: 0.9579\n",
            "Epoch 401/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0717 - acc: 0.9767 - val_loss: 0.1769 - val_acc: 0.9590\n",
            "Epoch 402/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0708 - acc: 0.9771 - val_loss: 0.1753 - val_acc: 0.9589\n",
            "Epoch 403/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0711 - acc: 0.9772 - val_loss: 0.1829 - val_acc: 0.9581\n",
            "Epoch 404/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0807 - acc: 0.9752 - val_loss: 0.1718 - val_acc: 0.9598\n",
            "Epoch 405/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0681 - acc: 0.9778 - val_loss: 0.1736 - val_acc: 0.9593\n",
            "Epoch 406/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0678 - acc: 0.9777 - val_loss: 0.1754 - val_acc: 0.9593\n",
            "Epoch 407/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0681 - acc: 0.9780 - val_loss: 0.1746 - val_acc: 0.9596\n",
            "Epoch 408/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0696 - acc: 0.9775 - val_loss: 0.1755 - val_acc: 0.9587\n",
            "Epoch 409/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0673 - acc: 0.9783 - val_loss: 0.1762 - val_acc: 0.9598\n",
            "Epoch 410/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0692 - acc: 0.9777 - val_loss: 0.1759 - val_acc: 0.9589\n",
            "Epoch 411/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0657 - acc: 0.9785 - val_loss: 0.1784 - val_acc: 0.9588\n",
            "Epoch 412/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0670 - acc: 0.9781 - val_loss: 0.1750 - val_acc: 0.9594\n",
            "Epoch 413/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0647 - acc: 0.9787 - val_loss: 0.2048 - val_acc: 0.9545\n",
            "Epoch 414/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0676 - acc: 0.9781 - val_loss: 0.1782 - val_acc: 0.9595\n",
            "Epoch 415/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0662 - acc: 0.9787 - val_loss: 0.1742 - val_acc: 0.9601\n",
            "Epoch 416/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0642 - acc: 0.9789 - val_loss: 0.1792 - val_acc: 0.9597\n",
            "Epoch 417/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0651 - acc: 0.9787 - val_loss: 0.1742 - val_acc: 0.9601\n",
            "Epoch 418/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0635 - acc: 0.9794 - val_loss: 0.1767 - val_acc: 0.9595\n",
            "Epoch 419/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0643 - acc: 0.9791 - val_loss: 0.1775 - val_acc: 0.9600\n",
            "Epoch 420/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0637 - acc: 0.9794 - val_loss: 0.1736 - val_acc: 0.9606\n",
            "Epoch 421/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0630 - acc: 0.9793 - val_loss: 0.1801 - val_acc: 0.9595\n",
            "Epoch 422/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0911 - acc: 0.9758 - val_loss: 0.1742 - val_acc: 0.9606\n",
            "Epoch 423/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0600 - acc: 0.9805 - val_loss: 0.1760 - val_acc: 0.9601\n",
            "Epoch 424/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0587 - acc: 0.9811 - val_loss: 0.1755 - val_acc: 0.9603\n",
            "Epoch 425/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0601 - acc: 0.9802 - val_loss: 0.1767 - val_acc: 0.9602\n",
            "Epoch 426/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0607 - acc: 0.9804 - val_loss: 0.1825 - val_acc: 0.9585\n",
            "Epoch 427/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0599 - acc: 0.9804 - val_loss: 0.1766 - val_acc: 0.9603\n",
            "Epoch 428/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0611 - acc: 0.9800 - val_loss: 0.1758 - val_acc: 0.9600\n",
            "Epoch 429/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0619 - acc: 0.9804 - val_loss: 0.1756 - val_acc: 0.9603\n",
            "Epoch 430/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0598 - acc: 0.9802 - val_loss: 0.1817 - val_acc: 0.9592\n",
            "Epoch 431/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0667 - acc: 0.9792 - val_loss: 0.1746 - val_acc: 0.9608\n",
            "Epoch 432/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0583 - acc: 0.9809 - val_loss: 0.1746 - val_acc: 0.9607\n",
            "Epoch 433/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0600 - acc: 0.9804 - val_loss: 0.1778 - val_acc: 0.9600\n",
            "Epoch 434/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0620 - acc: 0.9801 - val_loss: 0.1787 - val_acc: 0.9601\n",
            "Epoch 435/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0597 - acc: 0.9805 - val_loss: 0.1740 - val_acc: 0.9614\n",
            "Epoch 436/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0570 - acc: 0.9813 - val_loss: 0.1834 - val_acc: 0.9597\n",
            "Epoch 437/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0617 - acc: 0.9806 - val_loss: 0.1782 - val_acc: 0.9607\n",
            "Epoch 438/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0802 - acc: 0.9774 - val_loss: 0.1759 - val_acc: 0.9610\n",
            "Epoch 439/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0560 - acc: 0.9820 - val_loss: 0.1790 - val_acc: 0.9603\n",
            "Epoch 440/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0542 - acc: 0.9822 - val_loss: 0.1825 - val_acc: 0.9594\n",
            "Epoch 441/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0558 - acc: 0.9816 - val_loss: 0.1763 - val_acc: 0.9608\n",
            "Epoch 442/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0568 - acc: 0.9818 - val_loss: 0.1789 - val_acc: 0.9607\n",
            "Epoch 443/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0563 - acc: 0.9816 - val_loss: 0.1767 - val_acc: 0.9609\n",
            "Epoch 444/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0553 - acc: 0.9819 - val_loss: 0.1762 - val_acc: 0.9610\n",
            "Epoch 445/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0545 - acc: 0.9820 - val_loss: 0.1803 - val_acc: 0.9605\n",
            "Epoch 446/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0584 - acc: 0.9814 - val_loss: 0.1780 - val_acc: 0.9608\n",
            "Epoch 447/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0559 - acc: 0.9821 - val_loss: 0.1793 - val_acc: 0.9609\n",
            "Epoch 448/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0543 - acc: 0.9827 - val_loss: 0.1855 - val_acc: 0.9595\n",
            "Epoch 449/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0555 - acc: 0.9821 - val_loss: 0.1818 - val_acc: 0.9612\n",
            "Epoch 450/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0551 - acc: 0.9820 - val_loss: 0.1830 - val_acc: 0.9607\n",
            "Epoch 451/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0566 - acc: 0.9819 - val_loss: 0.1815 - val_acc: 0.9614\n",
            "Epoch 452/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0524 - acc: 0.9827 - val_loss: 0.1805 - val_acc: 0.9612\n",
            "Epoch 453/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0526 - acc: 0.9831 - val_loss: 0.1786 - val_acc: 0.9614\n",
            "Epoch 454/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0544 - acc: 0.9825 - val_loss: 0.1806 - val_acc: 0.9612\n",
            "Epoch 455/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0572 - acc: 0.9821 - val_loss: 0.1756 - val_acc: 0.9619\n",
            "Epoch 456/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0505 - acc: 0.9835 - val_loss: 0.1789 - val_acc: 0.9621\n",
            "Epoch 457/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0523 - acc: 0.9830 - val_loss: 0.1793 - val_acc: 0.9615\n",
            "Epoch 458/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0517 - acc: 0.9830 - val_loss: 0.1824 - val_acc: 0.9611\n",
            "Epoch 459/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0530 - acc: 0.9828 - val_loss: 0.1798 - val_acc: 0.9617\n",
            "Epoch 460/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0500 - acc: 0.9836 - val_loss: 0.1805 - val_acc: 0.9613\n",
            "Epoch 461/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0542 - acc: 0.9829 - val_loss: 0.1823 - val_acc: 0.9611\n",
            "Epoch 462/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0518 - acc: 0.9832 - val_loss: 0.1850 - val_acc: 0.9597\n",
            "Epoch 463/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0492 - acc: 0.9838 - val_loss: 0.1814 - val_acc: 0.9615\n",
            "Epoch 464/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0486 - acc: 0.9840 - val_loss: 0.1801 - val_acc: 0.9613\n",
            "Epoch 465/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0512 - acc: 0.9835 - val_loss: 0.1874 - val_acc: 0.9596\n",
            "Epoch 466/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0523 - acc: 0.9833 - val_loss: 0.1823 - val_acc: 0.9608\n",
            "Epoch 467/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0485 - acc: 0.9840 - val_loss: 0.1828 - val_acc: 0.9615\n",
            "Epoch 468/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0500 - acc: 0.9837 - val_loss: 0.1858 - val_acc: 0.9610\n",
            "Epoch 469/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0498 - acc: 0.9839 - val_loss: 0.1829 - val_acc: 0.9616\n",
            "Epoch 470/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0491 - acc: 0.9841 - val_loss: 0.1825 - val_acc: 0.9608\n",
            "Epoch 471/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0481 - acc: 0.9841 - val_loss: 0.1817 - val_acc: 0.9619\n",
            "Epoch 472/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0482 - acc: 0.9842 - val_loss: 0.1850 - val_acc: 0.9610\n",
            "Epoch 473/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0489 - acc: 0.9842 - val_loss: 0.1848 - val_acc: 0.9612\n",
            "Epoch 474/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0474 - acc: 0.9845 - val_loss: 0.1839 - val_acc: 0.9620\n",
            "Epoch 475/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0475 - acc: 0.9845 - val_loss: 0.1817 - val_acc: 0.9617\n",
            "Epoch 476/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0473 - acc: 0.9846 - val_loss: 0.1800 - val_acc: 0.9619\n",
            "Epoch 477/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0460 - acc: 0.9850 - val_loss: 0.1835 - val_acc: 0.9612\n",
            "Epoch 478/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0482 - acc: 0.9844 - val_loss: 0.1893 - val_acc: 0.9609\n",
            "Epoch 479/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0453 - acc: 0.9852 - val_loss: 0.1882 - val_acc: 0.9607\n",
            "Epoch 480/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0493 - acc: 0.9844 - val_loss: 0.1827 - val_acc: 0.9619\n",
            "Epoch 481/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0456 - acc: 0.9852 - val_loss: 0.1826 - val_acc: 0.9608\n",
            "Epoch 482/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0460 - acc: 0.9851 - val_loss: 0.1803 - val_acc: 0.9615\n",
            "Epoch 483/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0470 - acc: 0.9846 - val_loss: 0.1815 - val_acc: 0.9617\n",
            "Epoch 484/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0449 - acc: 0.9852 - val_loss: 0.1842 - val_acc: 0.9621\n",
            "Epoch 485/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0451 - acc: 0.9851 - val_loss: 0.1807 - val_acc: 0.9621\n",
            "Epoch 486/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0447 - acc: 0.9857 - val_loss: 0.1871 - val_acc: 0.9615\n",
            "Epoch 487/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0450 - acc: 0.9852 - val_loss: 0.1812 - val_acc: 0.9624\n",
            "Epoch 488/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0436 - acc: 0.9859 - val_loss: 0.1828 - val_acc: 0.9621\n",
            "Epoch 489/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0434 - acc: 0.9858 - val_loss: 0.1815 - val_acc: 0.9619\n",
            "Epoch 490/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0485 - acc: 0.9850 - val_loss: 0.1814 - val_acc: 0.9629\n",
            "Epoch 491/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0433 - acc: 0.9858 - val_loss: 0.1815 - val_acc: 0.9626\n",
            "Epoch 492/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0433 - acc: 0.9860 - val_loss: 0.1815 - val_acc: 0.9628\n",
            "Epoch 493/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0454 - acc: 0.9857 - val_loss: 0.1809 - val_acc: 0.9630\n",
            "Epoch 494/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0424 - acc: 0.9863 - val_loss: 0.1836 - val_acc: 0.9619\n",
            "Epoch 495/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0434 - acc: 0.9858 - val_loss: 0.1849 - val_acc: 0.9618\n",
            "Epoch 496/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0464 - acc: 0.9855 - val_loss: 0.1865 - val_acc: 0.9619\n",
            "Epoch 497/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0423 - acc: 0.9863 - val_loss: 0.1839 - val_acc: 0.9624\n",
            "Epoch 498/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0410 - acc: 0.9865 - val_loss: 0.1862 - val_acc: 0.9623\n",
            "Epoch 499/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0406 - acc: 0.9867 - val_loss: 0.1871 - val_acc: 0.9616\n",
            "Epoch 500/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0426 - acc: 0.9861 - val_loss: 0.1889 - val_acc: 0.9619\n",
            "Epoch 501/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0410 - acc: 0.9868 - val_loss: 0.1887 - val_acc: 0.9618\n",
            "Epoch 502/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0464 - acc: 0.9859 - val_loss: 0.2161 - val_acc: 0.9547\n",
            "Epoch 503/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0450 - acc: 0.9857 - val_loss: 0.1959 - val_acc: 0.9605\n",
            "Epoch 504/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0413 - acc: 0.9865 - val_loss: 0.1872 - val_acc: 0.9619\n",
            "Epoch 505/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0406 - acc: 0.9867 - val_loss: 0.1900 - val_acc: 0.9618\n",
            "Epoch 506/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0422 - acc: 0.9865 - val_loss: 0.1845 - val_acc: 0.9628\n",
            "Epoch 507/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0426 - acc: 0.9864 - val_loss: 0.1865 - val_acc: 0.9624\n",
            "Epoch 508/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0407 - acc: 0.9868 - val_loss: 0.1905 - val_acc: 0.9622\n",
            "Epoch 509/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0432 - acc: 0.9861 - val_loss: 0.1880 - val_acc: 0.9627\n",
            "Epoch 510/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0419 - acc: 0.9867 - val_loss: 0.1863 - val_acc: 0.9629\n",
            "Epoch 511/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0402 - acc: 0.9867 - val_loss: 0.1918 - val_acc: 0.9621\n",
            "Epoch 512/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0395 - acc: 0.9872 - val_loss: 0.1872 - val_acc: 0.9625\n",
            "Epoch 513/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0408 - acc: 0.9869 - val_loss: 0.1945 - val_acc: 0.9612\n",
            "Epoch 514/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0410 - acc: 0.9867 - val_loss: 0.1889 - val_acc: 0.9623\n",
            "Epoch 515/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0399 - acc: 0.9870 - val_loss: 0.1898 - val_acc: 0.9625\n",
            "Epoch 516/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0393 - acc: 0.9873 - val_loss: 0.1873 - val_acc: 0.9628\n",
            "Epoch 517/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0395 - acc: 0.9871 - val_loss: 0.1878 - val_acc: 0.9627\n",
            "Epoch 518/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0377 - acc: 0.9877 - val_loss: 0.1936 - val_acc: 0.9624\n",
            "Epoch 519/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0414 - acc: 0.9869 - val_loss: 0.1873 - val_acc: 0.9626\n",
            "Epoch 520/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0403 - acc: 0.9871 - val_loss: 0.1897 - val_acc: 0.9631\n",
            "Epoch 521/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0383 - acc: 0.9874 - val_loss: 0.1906 - val_acc: 0.9622\n",
            "Epoch 522/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0379 - acc: 0.9875 - val_loss: 0.1908 - val_acc: 0.9629\n",
            "Epoch 523/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0393 - acc: 0.9875 - val_loss: 0.1926 - val_acc: 0.9627\n",
            "Epoch 524/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0374 - acc: 0.9878 - val_loss: 0.1914 - val_acc: 0.9626\n",
            "Epoch 525/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0377 - acc: 0.9876 - val_loss: 0.1860 - val_acc: 0.9636\n",
            "Epoch 526/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0391 - acc: 0.9873 - val_loss: 0.1853 - val_acc: 0.9631\n",
            "Epoch 527/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0381 - acc: 0.9875 - val_loss: 0.1878 - val_acc: 0.9631\n",
            "Epoch 528/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0391 - acc: 0.9874 - val_loss: 0.1891 - val_acc: 0.9618\n",
            "Epoch 529/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0396 - acc: 0.9875 - val_loss: 0.1887 - val_acc: 0.9636\n",
            "Epoch 530/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0361 - acc: 0.9881 - val_loss: 0.1870 - val_acc: 0.9627\n",
            "Epoch 531/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0380 - acc: 0.9877 - val_loss: 0.1864 - val_acc: 0.9631\n",
            "Epoch 532/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0385 - acc: 0.9879 - val_loss: 0.1856 - val_acc: 0.9637\n",
            "Epoch 533/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0364 - acc: 0.9882 - val_loss: 0.1875 - val_acc: 0.9631\n",
            "Epoch 534/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0389 - acc: 0.9876 - val_loss: 0.1867 - val_acc: 0.9631\n",
            "Epoch 535/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0371 - acc: 0.9881 - val_loss: 0.1883 - val_acc: 0.9627\n",
            "Epoch 536/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0363 - acc: 0.9882 - val_loss: 0.1860 - val_acc: 0.9636\n",
            "Epoch 537/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0360 - acc: 0.9884 - val_loss: 0.1892 - val_acc: 0.9631\n",
            "Epoch 538/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0367 - acc: 0.9882 - val_loss: 0.1889 - val_acc: 0.9630\n",
            "Epoch 539/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0417 - acc: 0.9874 - val_loss: 0.1859 - val_acc: 0.9638\n",
            "Epoch 540/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0351 - acc: 0.9884 - val_loss: 0.1882 - val_acc: 0.9632\n",
            "Epoch 541/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0361 - acc: 0.9882 - val_loss: 0.1839 - val_acc: 0.9635\n",
            "Epoch 542/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0358 - acc: 0.9884 - val_loss: 0.1889 - val_acc: 0.9630\n",
            "Epoch 543/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0353 - acc: 0.9887 - val_loss: 0.1923 - val_acc: 0.9633\n",
            "Epoch 544/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0351 - acc: 0.9886 - val_loss: 0.1923 - val_acc: 0.9631\n",
            "Epoch 545/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0422 - acc: 0.9874 - val_loss: 0.1939 - val_acc: 0.9619\n",
            "Epoch 546/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0347 - acc: 0.9887 - val_loss: 0.1876 - val_acc: 0.9634\n",
            "Epoch 547/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0348 - acc: 0.9886 - val_loss: 0.1857 - val_acc: 0.9639\n",
            "Epoch 548/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0338 - acc: 0.9891 - val_loss: 0.1896 - val_acc: 0.9636\n",
            "Epoch 549/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0347 - acc: 0.9887 - val_loss: 0.1893 - val_acc: 0.9632\n",
            "Epoch 550/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0381 - acc: 0.9880 - val_loss: 0.1898 - val_acc: 0.9629\n",
            "Epoch 551/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0333 - acc: 0.9890 - val_loss: 0.1933 - val_acc: 0.9628\n",
            "Epoch 552/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0340 - acc: 0.9889 - val_loss: 0.1916 - val_acc: 0.9631\n",
            "Epoch 553/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0348 - acc: 0.9888 - val_loss: 0.1927 - val_acc: 0.9630\n",
            "Epoch 554/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0359 - acc: 0.9885 - val_loss: 0.1904 - val_acc: 0.9635\n",
            "Epoch 555/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0335 - acc: 0.9890 - val_loss: 0.1935 - val_acc: 0.9628\n",
            "Epoch 556/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0335 - acc: 0.9891 - val_loss: 0.1925 - val_acc: 0.9632\n",
            "Epoch 557/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0331 - acc: 0.9890 - val_loss: 0.1908 - val_acc: 0.9632\n",
            "Epoch 558/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0342 - acc: 0.9890 - val_loss: 0.1931 - val_acc: 0.9626\n",
            "Epoch 559/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0328 - acc: 0.9894 - val_loss: 0.1950 - val_acc: 0.9639\n",
            "Epoch 560/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0399 - acc: 0.9882 - val_loss: 0.1884 - val_acc: 0.9641\n",
            "Epoch 561/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0315 - acc: 0.9898 - val_loss: 0.1985 - val_acc: 0.9627\n",
            "Epoch 562/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0322 - acc: 0.9895 - val_loss: 0.1955 - val_acc: 0.9635\n",
            "Epoch 563/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0347 - acc: 0.9892 - val_loss: 0.1900 - val_acc: 0.9636\n",
            "Epoch 564/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0311 - acc: 0.9898 - val_loss: 0.1928 - val_acc: 0.9636\n",
            "Epoch 565/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0323 - acc: 0.9895 - val_loss: 0.1939 - val_acc: 0.9631\n",
            "Epoch 566/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0331 - acc: 0.9892 - val_loss: 0.1947 - val_acc: 0.9631\n",
            "Epoch 567/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0316 - acc: 0.9896 - val_loss: 0.1927 - val_acc: 0.9630\n",
            "Epoch 568/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0325 - acc: 0.9894 - val_loss: 0.1913 - val_acc: 0.9638\n",
            "Epoch 569/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0327 - acc: 0.9895 - val_loss: 0.1938 - val_acc: 0.9639\n",
            "Epoch 570/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0354 - acc: 0.9890 - val_loss: 0.1961 - val_acc: 0.9636\n",
            "Epoch 571/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0311 - acc: 0.9900 - val_loss: 0.1919 - val_acc: 0.9638\n",
            "Epoch 572/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0354 - acc: 0.9894 - val_loss: 0.1935 - val_acc: 0.9634\n",
            "Epoch 573/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0300 - acc: 0.9903 - val_loss: 0.1935 - val_acc: 0.9631\n",
            "Epoch 574/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0306 - acc: 0.9901 - val_loss: 0.1947 - val_acc: 0.9633\n",
            "Epoch 575/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0318 - acc: 0.9897 - val_loss: 0.1936 - val_acc: 0.9638\n",
            "Epoch 576/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0316 - acc: 0.9898 - val_loss: 0.1913 - val_acc: 0.9639\n",
            "Epoch 577/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0318 - acc: 0.9898 - val_loss: 0.1976 - val_acc: 0.9631\n",
            "Epoch 578/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0306 - acc: 0.9900 - val_loss: 0.1961 - val_acc: 0.9633\n",
            "Epoch 579/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0307 - acc: 0.9901 - val_loss: 0.1975 - val_acc: 0.9641\n",
            "Epoch 580/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0318 - acc: 0.9899 - val_loss: 0.1959 - val_acc: 0.9635\n",
            "Epoch 581/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0298 - acc: 0.9903 - val_loss: 0.1959 - val_acc: 0.9644\n",
            "Epoch 582/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0301 - acc: 0.9902 - val_loss: 0.1951 - val_acc: 0.9640\n",
            "Epoch 583/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0309 - acc: 0.9899 - val_loss: 0.1949 - val_acc: 0.9640\n",
            "Epoch 584/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0322 - acc: 0.9898 - val_loss: 0.1960 - val_acc: 0.9636\n",
            "Epoch 585/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0296 - acc: 0.9903 - val_loss: 0.1908 - val_acc: 0.9644\n",
            "Epoch 586/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0315 - acc: 0.9900 - val_loss: 0.1930 - val_acc: 0.9644\n",
            "Epoch 587/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0322 - acc: 0.9900 - val_loss: 0.1957 - val_acc: 0.9638\n",
            "Epoch 588/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0289 - acc: 0.9906 - val_loss: 0.1913 - val_acc: 0.9648\n",
            "Epoch 589/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1106 - acc: 0.9809 - val_loss: 0.1906 - val_acc: 0.9647\n",
            "Epoch 590/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0292 - acc: 0.9908 - val_loss: 0.1919 - val_acc: 0.9649\n",
            "Epoch 591/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0288 - acc: 0.9907 - val_loss: 0.1932 - val_acc: 0.9640\n",
            "Epoch 592/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0294 - acc: 0.9905 - val_loss: 0.1941 - val_acc: 0.9641\n",
            "Epoch 593/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0293 - acc: 0.9907 - val_loss: 0.1922 - val_acc: 0.9651\n",
            "Epoch 594/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.1161 - acc: 0.9811 - val_loss: 0.1906 - val_acc: 0.9645\n",
            "Epoch 595/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0288 - acc: 0.9907 - val_loss: 0.1938 - val_acc: 0.9645\n",
            "Epoch 596/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0275 - acc: 0.9911 - val_loss: 0.1943 - val_acc: 0.9644\n",
            "Epoch 597/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0305 - acc: 0.9903 - val_loss: 0.1962 - val_acc: 0.9639\n",
            "Epoch 598/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0285 - acc: 0.9910 - val_loss: 0.1975 - val_acc: 0.9647\n",
            "Epoch 599/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0280 - acc: 0.9909 - val_loss: 0.1946 - val_acc: 0.9644\n",
            "Epoch 600/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0288 - acc: 0.9909 - val_loss: 0.1959 - val_acc: 0.9647\n",
            "Epoch 601/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0299 - acc: 0.9905 - val_loss: 0.1962 - val_acc: 0.9640\n",
            "Epoch 602/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0288 - acc: 0.9908 - val_loss: 0.1978 - val_acc: 0.9644\n",
            "Epoch 603/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0280 - acc: 0.9908 - val_loss: 0.1963 - val_acc: 0.9637\n",
            "Epoch 604/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0281 - acc: 0.9908 - val_loss: 0.1971 - val_acc: 0.9641\n",
            "Epoch 605/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0290 - acc: 0.9908 - val_loss: 0.1964 - val_acc: 0.9641\n",
            "Epoch 606/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0295 - acc: 0.9906 - val_loss: 0.2031 - val_acc: 0.9639\n",
            "Epoch 607/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0289 - acc: 0.9908 - val_loss: 0.1949 - val_acc: 0.9646\n",
            "Epoch 608/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0279 - acc: 0.9911 - val_loss: 0.1992 - val_acc: 0.9638\n",
            "Epoch 609/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0280 - acc: 0.9911 - val_loss: 0.1949 - val_acc: 0.9647\n",
            "Epoch 610/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0285 - acc: 0.9908 - val_loss: 0.1971 - val_acc: 0.9640\n",
            "Epoch 611/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0273 - acc: 0.9910 - val_loss: 0.2006 - val_acc: 0.9636\n",
            "Epoch 612/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0294 - acc: 0.9908 - val_loss: 0.1975 - val_acc: 0.9640\n",
            "Epoch 613/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0274 - acc: 0.9911 - val_loss: 0.1951 - val_acc: 0.9643\n",
            "Epoch 614/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0277 - acc: 0.9912 - val_loss: 0.1964 - val_acc: 0.9645\n",
            "Epoch 615/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0271 - acc: 0.9911 - val_loss: 0.1982 - val_acc: 0.9645\n",
            "Epoch 616/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0282 - acc: 0.9909 - val_loss: 0.1976 - val_acc: 0.9641\n",
            "Epoch 617/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0265 - acc: 0.9915 - val_loss: 0.2001 - val_acc: 0.9641\n",
            "Epoch 618/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0294 - acc: 0.9908 - val_loss: 0.2000 - val_acc: 0.9640\n",
            "Epoch 619/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0266 - acc: 0.9915 - val_loss: 0.1998 - val_acc: 0.9641\n",
            "Epoch 620/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0296 - acc: 0.9910 - val_loss: 0.1923 - val_acc: 0.9650\n",
            "Epoch 621/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0485 - acc: 0.9888 - val_loss: 0.1989 - val_acc: 0.9636\n",
            "Epoch 622/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0278 - acc: 0.9911 - val_loss: 0.1962 - val_acc: 0.9644\n",
            "Epoch 623/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0256 - acc: 0.9919 - val_loss: 0.1954 - val_acc: 0.9648\n",
            "Epoch 624/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0256 - acc: 0.9918 - val_loss: 0.1967 - val_acc: 0.9647\n",
            "Epoch 625/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0259 - acc: 0.9917 - val_loss: 0.2025 - val_acc: 0.9640\n",
            "Epoch 626/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.1086 - acc: 0.9835 - val_loss: 0.1973 - val_acc: 0.9636\n",
            "Epoch 627/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0281 - acc: 0.9912 - val_loss: 0.1955 - val_acc: 0.9652\n",
            "Epoch 628/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0257 - acc: 0.9917 - val_loss: 0.1978 - val_acc: 0.9646\n",
            "Epoch 629/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0270 - acc: 0.9913 - val_loss: 0.1969 - val_acc: 0.9641\n",
            "Epoch 630/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0265 - acc: 0.9916 - val_loss: 0.1933 - val_acc: 0.9644\n",
            "Epoch 631/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0257 - acc: 0.9919 - val_loss: 0.1987 - val_acc: 0.9647\n",
            "Epoch 632/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0270 - acc: 0.9913 - val_loss: 0.2012 - val_acc: 0.9639\n",
            "Epoch 633/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0266 - acc: 0.9915 - val_loss: 0.2059 - val_acc: 0.9631\n",
            "Epoch 634/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0266 - acc: 0.9916 - val_loss: 0.1962 - val_acc: 0.9641\n",
            "Epoch 635/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0272 - acc: 0.9915 - val_loss: 0.1925 - val_acc: 0.9647\n",
            "Epoch 636/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0284 - acc: 0.9915 - val_loss: 0.1956 - val_acc: 0.9647\n",
            "Epoch 637/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0271 - acc: 0.9915 - val_loss: 0.1982 - val_acc: 0.9645\n",
            "Epoch 638/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0251 - acc: 0.9919 - val_loss: 0.1986 - val_acc: 0.9642\n",
            "Epoch 639/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0261 - acc: 0.9916 - val_loss: 0.1994 - val_acc: 0.9646\n",
            "Epoch 640/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0252 - acc: 0.9918 - val_loss: 0.2020 - val_acc: 0.9644\n",
            "Epoch 641/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0279 - acc: 0.9914 - val_loss: 0.1966 - val_acc: 0.9645\n",
            "Epoch 642/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0247 - acc: 0.9920 - val_loss: 0.2019 - val_acc: 0.9641\n",
            "Epoch 643/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0272 - acc: 0.9915 - val_loss: 0.2002 - val_acc: 0.9643\n",
            "Epoch 644/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0263 - acc: 0.9918 - val_loss: 0.1973 - val_acc: 0.9648\n",
            "Epoch 645/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0253 - acc: 0.9918 - val_loss: 0.1975 - val_acc: 0.9647\n",
            "Epoch 646/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0267 - acc: 0.9917 - val_loss: 0.1996 - val_acc: 0.9643\n",
            "Epoch 647/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0255 - acc: 0.9918 - val_loss: 0.1953 - val_acc: 0.9647\n",
            "Epoch 648/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0261 - acc: 0.9917 - val_loss: 0.2011 - val_acc: 0.9639\n",
            "Epoch 649/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0241 - acc: 0.9922 - val_loss: 0.1968 - val_acc: 0.9651\n",
            "Epoch 650/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0281 - acc: 0.9915 - val_loss: 0.1987 - val_acc: 0.9645\n",
            "Epoch 651/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0257 - acc: 0.9919 - val_loss: 0.1990 - val_acc: 0.9645\n",
            "Epoch 652/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0281 - acc: 0.9917 - val_loss: 0.1959 - val_acc: 0.9647\n",
            "Epoch 653/1000\n",
            "14030/14030 [==============================] - 58s 4ms/step - loss: 0.0240 - acc: 0.9923 - val_loss: 0.1968 - val_acc: 0.9656\n",
            "Epoch 654/1000\n",
            "14030/14030 [==============================] - 57s 4ms/step - loss: 0.0242 - acc: 0.9922 - val_loss: 0.1960 - val_acc: 0.9650\n",
            "Epoch 655/1000\n",
            " 3584/14030 [======>.......................] - ETA: 40s - loss: 0.0267 - acc: 0.9918"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZJEOlse1d_w-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7235
        },
        "outputId": "dc2a87ca-54c9-417c-e9f2-2753107f620a"
      },
      "cell_type": "code",
      "source": [
        "histio=model.fit([X1]+auxseq, Y, epochs=200, batch_size=512, validation_split=0.1)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 14030 samples, validate on 1559 samples\n",
            "Epoch 1/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.4126 - acc: 0.8749 - val_loss: 0.3444 - val_acc: 0.8972\n",
            "Epoch 2/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3575 - acc: 0.8896 - val_loss: 0.3420 - val_acc: 0.8988\n",
            "Epoch 3/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3940 - acc: 0.8790 - val_loss: 0.3344 - val_acc: 0.9002\n",
            "Epoch 4/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3559 - acc: 0.8898 - val_loss: 0.3513 - val_acc: 0.8970\n",
            "Epoch 5/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3854 - acc: 0.8810 - val_loss: 0.3365 - val_acc: 0.9004\n",
            "Epoch 6/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3499 - acc: 0.8913 - val_loss: 0.3309 - val_acc: 0.9027\n",
            "Epoch 7/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3780 - acc: 0.8848 - val_loss: 0.3262 - val_acc: 0.9033\n",
            "Epoch 8/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3449 - acc: 0.8927 - val_loss: 0.3230 - val_acc: 0.9034\n",
            "Epoch 9/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3558 - acc: 0.8896 - val_loss: 0.3201 - val_acc: 0.9055\n",
            "Epoch 10/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3595 - acc: 0.8894 - val_loss: 0.3216 - val_acc: 0.9044\n",
            "Epoch 11/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3320 - acc: 0.8967 - val_loss: 0.3444 - val_acc: 0.8976\n",
            "Epoch 12/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3618 - acc: 0.8886 - val_loss: 0.3153 - val_acc: 0.9062\n",
            "Epoch 13/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3280 - acc: 0.8979 - val_loss: 0.3133 - val_acc: 0.9070\n",
            "Epoch 14/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3452 - acc: 0.8931 - val_loss: 0.3107 - val_acc: 0.9080\n",
            "Epoch 15/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3244 - acc: 0.8988 - val_loss: 0.3188 - val_acc: 0.9052\n",
            "Epoch 16/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3500 - acc: 0.8920 - val_loss: 0.3083 - val_acc: 0.9074\n",
            "Epoch 17/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3158 - acc: 0.9018 - val_loss: 0.2990 - val_acc: 0.9121\n",
            "Epoch 18/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3522 - acc: 0.8913 - val_loss: 0.3029 - val_acc: 0.9102\n",
            "Epoch 19/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3096 - acc: 0.9036 - val_loss: 0.3117 - val_acc: 0.9087\n",
            "Epoch 20/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3185 - acc: 0.9004 - val_loss: 0.3009 - val_acc: 0.9125\n",
            "Epoch 21/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3330 - acc: 0.8974 - val_loss: 0.2998 - val_acc: 0.9116\n",
            "Epoch 22/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3030 - acc: 0.9056 - val_loss: 0.2968 - val_acc: 0.9119\n",
            "Epoch 23/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3311 - acc: 0.8979 - val_loss: 0.2943 - val_acc: 0.9129\n",
            "Epoch 24/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2975 - acc: 0.9075 - val_loss: 0.2904 - val_acc: 0.9144\n",
            "Epoch 25/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2959 - acc: 0.9075 - val_loss: 0.2922 - val_acc: 0.9140\n",
            "Epoch 26/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3015 - acc: 0.9058 - val_loss: 0.2987 - val_acc: 0.9125\n",
            "Epoch 27/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2979 - acc: 0.9069 - val_loss: 0.2933 - val_acc: 0.9124\n",
            "Epoch 28/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.3207 - acc: 0.9013 - val_loss: 0.2798 - val_acc: 0.9180\n",
            "Epoch 29/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2850 - acc: 0.9109 - val_loss: 0.2788 - val_acc: 0.9185\n",
            "Epoch 30/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2884 - acc: 0.9099 - val_loss: 0.2801 - val_acc: 0.9172\n",
            "Epoch 31/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2885 - acc: 0.9099 - val_loss: 0.2775 - val_acc: 0.9183\n",
            "Epoch 32/200\n",
            "14030/14030 [==============================] - 63s 4ms/step - loss: 0.2852 - acc: 0.9111 - val_loss: 0.2768 - val_acc: 0.9188\n",
            "Epoch 33/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2802 - acc: 0.9122 - val_loss: 0.3037 - val_acc: 0.9120\n",
            "Epoch 34/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2782 - acc: 0.9132 - val_loss: 0.2809 - val_acc: 0.9175\n",
            "Epoch 35/200\n",
            "14030/14030 [==============================] - 63s 4ms/step - loss: 0.2925 - acc: 0.9090 - val_loss: 0.2754 - val_acc: 0.9184\n",
            "Epoch 36/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2720 - acc: 0.9146 - val_loss: 0.2930 - val_acc: 0.9137\n",
            "Epoch 37/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2906 - acc: 0.9097 - val_loss: 0.2747 - val_acc: 0.9199\n",
            "Epoch 38/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2693 - acc: 0.9152 - val_loss: 0.2672 - val_acc: 0.9217\n",
            "Epoch 39/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2894 - acc: 0.9096 - val_loss: 0.2704 - val_acc: 0.9203\n",
            "Epoch 40/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2615 - acc: 0.9177 - val_loss: 0.2754 - val_acc: 0.9183\n",
            "Epoch 41/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2673 - acc: 0.9163 - val_loss: 0.2660 - val_acc: 0.9233\n",
            "Epoch 42/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2799 - acc: 0.9127 - val_loss: 0.2604 - val_acc: 0.9237\n",
            "Epoch 43/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2614 - acc: 0.9176 - val_loss: 0.2623 - val_acc: 0.9233\n",
            "Epoch 44/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2727 - acc: 0.9157 - val_loss: 0.2592 - val_acc: 0.9238\n",
            "Epoch 45/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2498 - acc: 0.9215 - val_loss: 0.2603 - val_acc: 0.9241\n",
            "Epoch 46/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2565 - acc: 0.9191 - val_loss: 0.2708 - val_acc: 0.9204\n",
            "Epoch 47/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2485 - acc: 0.9216 - val_loss: 0.2708 - val_acc: 0.9204\n",
            "Epoch 48/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2775 - acc: 0.9150 - val_loss: 0.2555 - val_acc: 0.9245\n",
            "Epoch 49/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2448 - acc: 0.9228 - val_loss: 0.2597 - val_acc: 0.9239\n",
            "Epoch 50/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2503 - acc: 0.9213 - val_loss: 0.2652 - val_acc: 0.9225\n",
            "Epoch 51/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2477 - acc: 0.9221 - val_loss: 0.2555 - val_acc: 0.9252\n",
            "Epoch 52/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2445 - acc: 0.9229 - val_loss: 0.2556 - val_acc: 0.9249\n",
            "Epoch 53/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2398 - acc: 0.9243 - val_loss: 0.2515 - val_acc: 0.9263\n",
            "Epoch 54/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2531 - acc: 0.9211 - val_loss: 0.2482 - val_acc: 0.9274\n",
            "Epoch 55/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2340 - acc: 0.9261 - val_loss: 0.2478 - val_acc: 0.9279\n",
            "Epoch 56/200\n",
            "14030/14030 [==============================] - 63s 4ms/step - loss: 0.2848 - acc: 0.9158 - val_loss: 0.2439 - val_acc: 0.9288\n",
            "Epoch 57/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2288 - acc: 0.9277 - val_loss: 0.2435 - val_acc: 0.9283\n",
            "Epoch 58/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2360 - acc: 0.9253 - val_loss: 0.2500 - val_acc: 0.9266\n",
            "Epoch 59/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2266 - acc: 0.9280 - val_loss: 0.2593 - val_acc: 0.9237\n",
            "Epoch 60/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2346 - acc: 0.9261 - val_loss: 0.2446 - val_acc: 0.9289\n",
            "Epoch 61/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2294 - acc: 0.9273 - val_loss: 0.2409 - val_acc: 0.9298\n",
            "Epoch 62/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2307 - acc: 0.9276 - val_loss: 0.2409 - val_acc: 0.9298\n",
            "Epoch 63/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2252 - acc: 0.9291 - val_loss: 0.2457 - val_acc: 0.9282\n",
            "Epoch 64/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2295 - acc: 0.9274 - val_loss: 0.2399 - val_acc: 0.9313\n",
            "Epoch 65/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2194 - acc: 0.9304 - val_loss: 0.2570 - val_acc: 0.9248\n",
            "Epoch 66/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2324 - acc: 0.9273 - val_loss: 0.2437 - val_acc: 0.9288\n",
            "Epoch 67/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2162 - acc: 0.9317 - val_loss: 0.2421 - val_acc: 0.9306\n",
            "Epoch 68/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2465 - acc: 0.9246 - val_loss: 0.2342 - val_acc: 0.9316\n",
            "Epoch 69/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2169 - acc: 0.9311 - val_loss: 0.2467 - val_acc: 0.9278\n",
            "Epoch 70/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2106 - acc: 0.9331 - val_loss: 0.2362 - val_acc: 0.9314\n",
            "Epoch 71/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2119 - acc: 0.9326 - val_loss: 0.2383 - val_acc: 0.9296\n",
            "Epoch 72/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2197 - acc: 0.9308 - val_loss: 0.2432 - val_acc: 0.9299\n",
            "Epoch 73/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2086 - acc: 0.9338 - val_loss: 0.2351 - val_acc: 0.9313\n",
            "Epoch 74/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2211 - acc: 0.9300 - val_loss: 0.2312 - val_acc: 0.9331\n",
            "Epoch 75/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2053 - acc: 0.9348 - val_loss: 0.2335 - val_acc: 0.9319\n",
            "Epoch 76/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2116 - acc: 0.9330 - val_loss: 0.3502 - val_acc: 0.8995\n",
            "Epoch 77/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2096 - acc: 0.9334 - val_loss: 0.2319 - val_acc: 0.9336\n",
            "Epoch 78/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2044 - acc: 0.9349 - val_loss: 0.2429 - val_acc: 0.9297\n",
            "Epoch 79/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2027 - acc: 0.9353 - val_loss: 0.2793 - val_acc: 0.9205\n",
            "Epoch 80/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2023 - acc: 0.9356 - val_loss: 0.2486 - val_acc: 0.9280\n",
            "Epoch 81/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2018 - acc: 0.9358 - val_loss: 0.2542 - val_acc: 0.9263\n",
            "Epoch 82/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2001 - acc: 0.9363 - val_loss: 0.2317 - val_acc: 0.9337\n",
            "Epoch 83/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1963 - acc: 0.9374 - val_loss: 0.2353 - val_acc: 0.9328\n",
            "Epoch 84/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2159 - acc: 0.9329 - val_loss: 0.2302 - val_acc: 0.9341\n",
            "Epoch 85/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1927 - acc: 0.9384 - val_loss: 0.2275 - val_acc: 0.9348\n",
            "Epoch 86/200\n",
            "14030/14030 [==============================] - 63s 4ms/step - loss: 0.3366 - acc: 0.9187 - val_loss: 0.2200 - val_acc: 0.9364\n",
            "Epoch 87/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1870 - acc: 0.9407 - val_loss: 0.2258 - val_acc: 0.9348\n",
            "Epoch 88/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1931 - acc: 0.9383 - val_loss: 0.2212 - val_acc: 0.9360\n",
            "Epoch 89/200\n",
            "14030/14030 [==============================] - 63s 4ms/step - loss: 0.1945 - acc: 0.9382 - val_loss: 0.2201 - val_acc: 0.9369\n",
            "Epoch 90/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1862 - acc: 0.9404 - val_loss: 0.2218 - val_acc: 0.9361\n",
            "Epoch 91/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1923 - acc: 0.9387 - val_loss: 0.5747 - val_acc: 0.8662\n",
            "Epoch 92/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.2010 - acc: 0.9379 - val_loss: 0.2257 - val_acc: 0.9353\n",
            "Epoch 93/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1862 - acc: 0.9404 - val_loss: 0.2185 - val_acc: 0.9367\n",
            "Epoch 94/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1849 - acc: 0.9407 - val_loss: 0.2196 - val_acc: 0.9369\n",
            "Epoch 95/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1883 - acc: 0.9400 - val_loss: 0.2179 - val_acc: 0.9377\n",
            "Epoch 96/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1768 - acc: 0.9434 - val_loss: 0.2282 - val_acc: 0.9345\n",
            "Epoch 97/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1859 - acc: 0.9407 - val_loss: 0.2361 - val_acc: 0.9325\n",
            "Epoch 98/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1764 - acc: 0.9433 - val_loss: 0.2181 - val_acc: 0.9373\n",
            "Epoch 99/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1881 - acc: 0.9403 - val_loss: 0.2186 - val_acc: 0.9377\n",
            "Epoch 100/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1726 - acc: 0.9448 - val_loss: 0.2268 - val_acc: 0.9354\n",
            "Epoch 101/200\n",
            "14030/14030 [==============================] - 63s 4ms/step - loss: 0.1796 - acc: 0.9425 - val_loss: 0.2202 - val_acc: 0.9372\n",
            "Epoch 102/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1754 - acc: 0.9437 - val_loss: 0.2381 - val_acc: 0.9336\n",
            "Epoch 103/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1726 - acc: 0.9448 - val_loss: 0.2276 - val_acc: 0.9357\n",
            "Epoch 104/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1743 - acc: 0.9444 - val_loss: 0.2186 - val_acc: 0.9393\n",
            "Epoch 105/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1859 - acc: 0.9412 - val_loss: 0.2216 - val_acc: 0.9372\n",
            "Epoch 106/200\n",
            "14030/14030 [==============================] - 63s 4ms/step - loss: 0.1681 - acc: 0.9457 - val_loss: 0.2156 - val_acc: 0.9385\n",
            "Epoch 107/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1668 - acc: 0.9468 - val_loss: 0.2426 - val_acc: 0.9319\n",
            "Epoch 108/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1735 - acc: 0.9447 - val_loss: 0.2138 - val_acc: 0.9397\n",
            "Epoch 109/200\n",
            "14030/14030 [==============================] - 63s 4ms/step - loss: 0.1669 - acc: 0.9463 - val_loss: 0.2140 - val_acc: 0.9387\n",
            "Epoch 110/200\n",
            "14030/14030 [==============================] - 63s 4ms/step - loss: 0.1718 - acc: 0.9451 - val_loss: 0.2130 - val_acc: 0.9395\n",
            "Epoch 111/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1659 - acc: 0.9465 - val_loss: 0.2137 - val_acc: 0.9403\n",
            "Epoch 112/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1634 - acc: 0.9475 - val_loss: 0.2165 - val_acc: 0.9393\n",
            "Epoch 113/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1761 - acc: 0.9438 - val_loss: 0.2256 - val_acc: 0.9373\n",
            "Epoch 114/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1611 - acc: 0.9486 - val_loss: 0.2114 - val_acc: 0.9405\n",
            "Epoch 115/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1654 - acc: 0.9468 - val_loss: 0.2101 - val_acc: 0.9416\n",
            "Epoch 116/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1609 - acc: 0.9482 - val_loss: 0.2123 - val_acc: 0.9413\n",
            "Epoch 117/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1599 - acc: 0.9488 - val_loss: 0.2147 - val_acc: 0.9400\n",
            "Epoch 118/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1572 - acc: 0.9494 - val_loss: 0.2126 - val_acc: 0.9399\n",
            "Epoch 119/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1585 - acc: 0.9486 - val_loss: 0.2130 - val_acc: 0.9410\n",
            "Epoch 120/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1564 - acc: 0.9498 - val_loss: 0.2082 - val_acc: 0.9416\n",
            "Epoch 121/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1614 - acc: 0.9484 - val_loss: 0.2056 - val_acc: 0.9425\n",
            "Epoch 122/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1541 - acc: 0.9503 - val_loss: 0.2133 - val_acc: 0.9414\n",
            "Epoch 123/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1548 - acc: 0.9500 - val_loss: 0.2205 - val_acc: 0.9376\n",
            "Epoch 124/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1549 - acc: 0.9501 - val_loss: 0.2087 - val_acc: 0.9414\n",
            "Epoch 125/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1531 - acc: 0.9509 - val_loss: 0.2079 - val_acc: 0.9425\n",
            "Epoch 126/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1678 - acc: 0.9477 - val_loss: 0.2056 - val_acc: 0.9428\n",
            "Epoch 127/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1469 - acc: 0.9527 - val_loss: 0.2075 - val_acc: 0.9437\n",
            "Epoch 128/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1501 - acc: 0.9515 - val_loss: 0.2109 - val_acc: 0.9423\n",
            "Epoch 129/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1658 - acc: 0.9481 - val_loss: 0.2055 - val_acc: 0.9429\n",
            "Epoch 130/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1443 - acc: 0.9533 - val_loss: 0.2106 - val_acc: 0.9419\n",
            "Epoch 131/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1453 - acc: 0.9532 - val_loss: 0.2059 - val_acc: 0.9432\n",
            "Epoch 132/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1469 - acc: 0.9526 - val_loss: 0.2076 - val_acc: 0.9425\n",
            "Epoch 133/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1452 - acc: 0.9533 - val_loss: 0.2036 - val_acc: 0.9439\n",
            "Epoch 134/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1422 - acc: 0.9541 - val_loss: 0.2414 - val_acc: 0.9337\n",
            "Epoch 135/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1480 - acc: 0.9525 - val_loss: 0.2057 - val_acc: 0.9432\n",
            "Epoch 136/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1462 - acc: 0.9528 - val_loss: 0.2048 - val_acc: 0.9441\n",
            "Epoch 137/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1386 - acc: 0.9552 - val_loss: 0.2080 - val_acc: 0.9429\n",
            "Epoch 138/200\n",
            "14030/14030 [==============================] - 63s 4ms/step - loss: 0.1422 - acc: 0.9542 - val_loss: 0.2134 - val_acc: 0.9415\n",
            "Epoch 139/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1507 - acc: 0.9527 - val_loss: 0.2012 - val_acc: 0.9443\n",
            "Epoch 140/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1374 - acc: 0.9554 - val_loss: 0.2201 - val_acc: 0.9393\n",
            "Epoch 141/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1385 - acc: 0.9552 - val_loss: 0.2224 - val_acc: 0.9395\n",
            "Epoch 142/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1435 - acc: 0.9540 - val_loss: 0.2001 - val_acc: 0.9445\n",
            "Epoch 143/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1357 - acc: 0.9559 - val_loss: 0.1994 - val_acc: 0.9454\n",
            "Epoch 144/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1387 - acc: 0.9549 - val_loss: 0.2034 - val_acc: 0.9442\n",
            "Epoch 145/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1358 - acc: 0.9563 - val_loss: 0.2049 - val_acc: 0.9437\n",
            "Epoch 146/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1337 - acc: 0.9569 - val_loss: 0.2066 - val_acc: 0.9444\n",
            "Epoch 147/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1342 - acc: 0.9566 - val_loss: 0.2008 - val_acc: 0.9456\n",
            "Epoch 148/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1327 - acc: 0.9571 - val_loss: 0.2043 - val_acc: 0.9438\n",
            "Epoch 149/200\n",
            "14030/14030 [==============================] - 63s 4ms/step - loss: 0.1373 - acc: 0.9557 - val_loss: 0.1985 - val_acc: 0.9457\n",
            "Epoch 150/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1362 - acc: 0.9564 - val_loss: 0.2001 - val_acc: 0.9455\n",
            "Epoch 151/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1285 - acc: 0.9586 - val_loss: 0.2008 - val_acc: 0.9458\n",
            "Epoch 152/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1378 - acc: 0.9563 - val_loss: 0.1986 - val_acc: 0.9462\n",
            "Epoch 153/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1271 - acc: 0.9587 - val_loss: 0.1990 - val_acc: 0.9462\n",
            "Epoch 154/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1296 - acc: 0.9580 - val_loss: 0.1961 - val_acc: 0.9461\n",
            "Epoch 155/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1298 - acc: 0.9586 - val_loss: 0.2071 - val_acc: 0.9433\n",
            "Epoch 156/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1303 - acc: 0.9577 - val_loss: 0.2010 - val_acc: 0.9467\n",
            "Epoch 157/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1278 - acc: 0.9588 - val_loss: 0.1990 - val_acc: 0.9465\n",
            "Epoch 158/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1241 - acc: 0.9600 - val_loss: 0.2475 - val_acc: 0.9386\n",
            "Epoch 159/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1329 - acc: 0.9583 - val_loss: 0.2067 - val_acc: 0.9450\n",
            "Epoch 160/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1270 - acc: 0.9591 - val_loss: 0.1950 - val_acc: 0.9481\n",
            "Epoch 161/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1246 - acc: 0.9596 - val_loss: 0.1979 - val_acc: 0.9466\n",
            "Epoch 162/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1221 - acc: 0.9605 - val_loss: 0.2022 - val_acc: 0.9462\n",
            "Epoch 163/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1262 - acc: 0.9592 - val_loss: 0.1963 - val_acc: 0.9463\n",
            "Epoch 164/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1255 - acc: 0.9595 - val_loss: 0.1937 - val_acc: 0.9487\n",
            "Epoch 165/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1194 - acc: 0.9614 - val_loss: 0.1982 - val_acc: 0.9473\n",
            "Epoch 166/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1217 - acc: 0.9608 - val_loss: 0.1994 - val_acc: 0.9482\n",
            "Epoch 167/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1180 - acc: 0.9619 - val_loss: 0.1948 - val_acc: 0.9482\n",
            "Epoch 168/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1189 - acc: 0.9619 - val_loss: 0.1987 - val_acc: 0.9477\n",
            "Epoch 169/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1179 - acc: 0.9617 - val_loss: 0.1916 - val_acc: 0.9490\n",
            "Epoch 170/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1170 - acc: 0.9625 - val_loss: 0.2001 - val_acc: 0.9476\n",
            "Epoch 171/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1233 - acc: 0.9603 - val_loss: 0.1920 - val_acc: 0.9498\n",
            "Epoch 172/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1146 - acc: 0.9631 - val_loss: 0.1931 - val_acc: 0.9491\n",
            "Epoch 173/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1143 - acc: 0.9631 - val_loss: 0.2062 - val_acc: 0.9461\n",
            "Epoch 174/200\n",
            "14030/14030 [==============================] - 63s 4ms/step - loss: 0.1174 - acc: 0.9618 - val_loss: 0.1952 - val_acc: 0.9485\n",
            "Epoch 175/200\n",
            "14030/14030 [==============================] - 63s 4ms/step - loss: 0.1268 - acc: 0.9600 - val_loss: 0.1938 - val_acc: 0.9487\n",
            "Epoch 176/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1103 - acc: 0.9642 - val_loss: 0.1956 - val_acc: 0.9487\n",
            "Epoch 177/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1129 - acc: 0.9632 - val_loss: 0.1963 - val_acc: 0.9492\n",
            "Epoch 178/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1125 - acc: 0.9635 - val_loss: 0.2028 - val_acc: 0.9466\n",
            "Epoch 179/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1114 - acc: 0.9636 - val_loss: 0.1992 - val_acc: 0.9477\n",
            "Epoch 180/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1118 - acc: 0.9638 - val_loss: 0.1939 - val_acc: 0.9507\n",
            "Epoch 181/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1134 - acc: 0.9633 - val_loss: 0.1923 - val_acc: 0.9499\n",
            "Epoch 182/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1096 - acc: 0.9647 - val_loss: 0.1961 - val_acc: 0.9493\n",
            "Epoch 183/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1079 - acc: 0.9654 - val_loss: 0.1927 - val_acc: 0.9497\n",
            "Epoch 184/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1112 - acc: 0.9643 - val_loss: 0.1921 - val_acc: 0.9503\n",
            "Epoch 185/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1104 - acc: 0.9644 - val_loss: 0.1917 - val_acc: 0.9503\n",
            "Epoch 186/200\n",
            "14030/14030 [==============================] - 63s 4ms/step - loss: 0.1074 - acc: 0.9651 - val_loss: 0.1922 - val_acc: 0.9503\n",
            "Epoch 187/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1057 - acc: 0.9655 - val_loss: 0.1936 - val_acc: 0.9497\n",
            "Epoch 188/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1085 - acc: 0.9649 - val_loss: 0.1906 - val_acc: 0.9514\n",
            "Epoch 189/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1051 - acc: 0.9662 - val_loss: 0.2009 - val_acc: 0.9485\n",
            "Epoch 190/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1102 - acc: 0.9643 - val_loss: 0.1892 - val_acc: 0.9511\n",
            "Epoch 191/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1046 - acc: 0.9663 - val_loss: 0.1908 - val_acc: 0.9514\n",
            "Epoch 192/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1056 - acc: 0.9656 - val_loss: 0.1945 - val_acc: 0.9502\n",
            "Epoch 193/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1041 - acc: 0.9665 - val_loss: 0.2006 - val_acc: 0.9478\n",
            "Epoch 194/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1065 - acc: 0.9659 - val_loss: 0.1907 - val_acc: 0.9510\n",
            "Epoch 195/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1019 - acc: 0.9668 - val_loss: 0.1922 - val_acc: 0.9507\n",
            "Epoch 196/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1047 - acc: 0.9664 - val_loss: 0.1947 - val_acc: 0.9511\n",
            "Epoch 197/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1035 - acc: 0.9665 - val_loss: 0.1916 - val_acc: 0.9511\n",
            "Epoch 198/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.0975 - acc: 0.9684 - val_loss: 0.1912 - val_acc: 0.9508\n",
            "Epoch 199/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.1106 - acc: 0.9646 - val_loss: 0.1903 - val_acc: 0.9519\n",
            "Epoch 200/200\n",
            "14030/14030 [==============================] - 62s 4ms/step - loss: 0.0994 - acc: 0.9678 - val_loss: 0.1992 - val_acc: 0.9494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8ZsZUqvTZ0cT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "histiok=model.fit([X1]+auxseq, Y, epochs=200, batch_size=512, validation_split=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}